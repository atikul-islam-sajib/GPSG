{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_6/fpk79cm53rxgcj2gh5prtww00000gn/T/ipykernel_2825/856264201.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "\n",
    "mnist_data = datasets.MNIST(\n",
    "    root=\"data/\", train=True, transform=transforms.ToTensor(), download=True\n",
    ")\n",
    "dataloader = DataLoader(mnist_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of this dataset: torch.Size([32, 1, 28, 28]) \n",
      "# of the dataset that I downloaded = 60000 \n"
     ]
    }
   ],
   "source": [
    "#Check the dataset\n",
    "total_train = 0\n",
    "for features, label in dataloader:\n",
    "    total_train = total_train + features.shape[0]\n",
    "\n",
    "print(\"The shape of this dataset: {} \".format(features.shape))\n",
    "print(\"# of the dataset that I downloaded = {} \".format(total_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 784])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.reshape(-1, 28*28).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    A generative neural network model for generating images using the DCGAN architecture.\n",
    "\n",
    "    Args:\n",
    "        latent_space (int): The dimensionality of the latent space noise vector. Default is 100.\n",
    "\n",
    "    Attributes:\n",
    "        latent_space (int): The dimensionality of the latent space noise vector.\n",
    "        model (nn.Sequential): The generator model composed of several layers.\n",
    "\n",
    "    Example:\n",
    "        >>> generator = Generator(latent_space=100)\n",
    "        >>> noise = torch.randn(64, 100)\n",
    "        >>> generated_images = generator(noise)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_space=100):\n",
    "        \"\"\"\n",
    "        Initialize the Generator.\n",
    "\n",
    "        Args:\n",
    "            latent_space (int, optional): The dimensionality of the latent space noise vector. Default is 100.\n",
    "        \"\"\"\n",
    "        self.latent_space = latent_space\n",
    "        super(Generator, self).__init__()\n",
    "        layers_config = [\n",
    "            (self.latent_space, 256, 0.2),\n",
    "            (256, 512, 0.2),\n",
    "            (512, 1024, 0.2),\n",
    "            (1024, 28 * 28),\n",
    "        ]\n",
    "        self.model = self.generate_layer(layers_config=layers_config)\n",
    "\n",
    "    def generate_layer(self, layers_config):\n",
    "        \"\"\"\n",
    "        Create the layers of the generator model based on the provided configuration.\n",
    "\n",
    "        Args:\n",
    "            layers_config (list): A list of tuples specifying the layer configurations.\n",
    "\n",
    "        Returns:\n",
    "            nn.Sequential: A sequential model containing the specified layers.\n",
    "\n",
    "        Example:\n",
    "            >>> layers_config = [(100, 256, 0.02), (256, 512, 0.02), (512, 1024, 0.02), (1024, 28*28)]\n",
    "            >>> generator = Generator()\n",
    "            >>> generator_model = generator.generate_layer(layers_config)\n",
    "        \"\"\"\n",
    "        layers = OrderedDict()\n",
    "        for index, (input_feature, out_feature, negative_slope) in enumerate(\n",
    "            layers_config[:-1]\n",
    "        ):\n",
    "            layers[f\"layer_{index}\"] = nn.Linear(\n",
    "                in_features=input_feature, out_features=out_feature\n",
    "            )\n",
    "            layers[f\"layer_{index}_activation\"] = nn.LeakyReLU(\n",
    "                negative_slope=negative_slope\n",
    "            )\n",
    "\n",
    "        layers[f\"output_layer\"] = nn.Linear(\n",
    "            in_features=layers_config[-1][0], out_features=layers_config[-1][1]\n",
    "        )\n",
    "        layers[f\"output_layer_activation\"] = nn.Tanh()\n",
    "\n",
    "        return nn.Sequential(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the generator model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input noise tensor sampled from the latent space.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Generated images.\n",
    "\n",
    "        Example:\n",
    "            >>> noise = torch.randn(64, 100)\n",
    "            >>> generated_images = generator(noise)\n",
    "        \"\"\"\n",
    "        if x is not None:\n",
    "            x = self.model(x)\n",
    "        else:\n",
    "            x = \"ERROR\"\n",
    "\n",
    "        return x.reshape(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of Generator(\n",
      "  (model): Sequential(\n",
      "    (layer_0): Linear(in_features=100, out_features=256, bias=True)\n",
      "    (layer_0_activation): LeakyReLU(negative_slope=0.2)\n",
      "    (layer_1): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (layer_1_activation): LeakyReLU(negative_slope=0.2)\n",
      "    (layer_2): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (layer_2_activation): LeakyReLU(negative_slope=0.2)\n",
      "    (output_layer): Linear(in_features=1024, out_features=784, bias=True)\n",
      "    (output_layer_activation): Tanh()\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "generator = Generator()\n",
    "\n",
    "print(generator.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: model.layer_0.weight & # of parameters: 25600 \n",
      "Layer: model.layer_0.bias & # of parameters: 256 \n",
      "Layer: model.layer_1.weight & # of parameters: 131072 \n",
      "Layer: model.layer_1.bias & # of parameters: 512 \n",
      "Layer: model.layer_2.weight & # of parameters: 524288 \n",
      "Layer: model.layer_2.bias & # of parameters: 1024 \n",
      "Layer: model.output_layer.weight & # of parameters: 802816 \n",
      "Layer: model.output_layer.bias & # of parameters: 784 \n",
      "\n",
      "TOTAL NUMBER OF PARAMETERS OF GENERATOR IS 1486352 \n"
     ]
    }
   ],
   "source": [
    "# Total number of parameters of Generator\n",
    "\n",
    "total_parameters = 0\n",
    "for layer, params in generator.named_parameters():\n",
    "    total_parameters += params.numel()\n",
    "    print(\"Layer: {} & # of parameters: {} \".format(layer, params.numel()))\n",
    "\n",
    "\n",
    "print(\n",
    "    \"\\nTotal number of parameters of generator is {} \".format(total_parameters).upper()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    A Discriminator class representing a neural network model for distinguishing real images from generated ones.\n",
    "\n",
    "    This class inherits from nn.Module and constructs a neural network discriminator model suitable for a Generative\n",
    "    Adversarial Network (GAN). The discriminator is designed to take flattened image inputs (such as those from the\n",
    "    MNIST dataset) and output a single value indicating the likelihood that the image is real.\n",
    "\n",
    "    Attributes:\n",
    "        model (torch.nn.Sequential): A sequential container of layers forming the discriminator network. The architecture\n",
    "                                     is defined based on the layers configuration provided in `layers_config`.\n",
    "\n",
    "    Methods:\n",
    "        forward(x): Defines the forward pass of the discriminator.\n",
    "\n",
    "    Parameters:\n",
    "        layers_config (list of tuples): Each tuple in the list contains configuration for a layer in the model,\n",
    "                                        including the number of input features, output features, and the negative\n",
    "                                        slope for the LeakyReLU activation function. The last layer uses a Sigmoid\n",
    "                                        activation function instead of LeakyReLU.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        layers_config = [\n",
    "            (28 * 28, 512, 0.2),\n",
    "            (512, 256, 0.2),\n",
    "            (256, 1),\n",
    "        ]\n",
    "        self.model = self.discriminator_block(layers_config)\n",
    "\n",
    "    def discriminator_block(self, layers_config):\n",
    "        \"\"\"\n",
    "        Builds the discriminator block based on the provided layers configuration.\n",
    "\n",
    "        Args:\n",
    "            layers_config (list of tuples): Configuration for each layer in the discriminator model.\n",
    "\n",
    "        Returns:\n",
    "            torch.nn.Sequential: A sequential container of layers forming the discriminator network.\n",
    "        \"\"\"\n",
    "        layers = OrderedDict()\n",
    "        for index, (input_features, output_features, negative_slope) in enumerate(\n",
    "            layers_config[:-1]\n",
    "        ):\n",
    "            layers[f\"{index}_layer\"] = nn.Linear(\n",
    "                in_features=input_features, out_features=output_features\n",
    "            )\n",
    "            layers[f\"{index}_activation\"] = nn.LeakyReLU(negative_slope=negative_slope)\n",
    "\n",
    "        # Output layer with Sigmoid activation\n",
    "        layers[\"output_layer\"] = nn.Linear(\n",
    "            in_features=layers_config[-1][0], out_features=layers_config[-1][1]\n",
    "        )\n",
    "        layers[\"output_activation\"] = nn.Sigmoid()\n",
    "\n",
    "        return nn.Sequential(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the discriminator.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor containing the image data.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output of the discriminator, representing the probability that the input image is real.\n",
    "        \"\"\"\n",
    "        if x is not None:\n",
    "            x = x.view(-1, 28 * 28)\n",
    "            x = self.model(x)\n",
    "        else:\n",
    "            x = \"ERROR\"\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of Discriminator(\n",
      "  (model): Sequential(\n",
      "    (0_layer): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (0_activation): LeakyReLU(negative_slope=0.2)\n",
      "    (1_layer): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (1_activation): LeakyReLU(negative_slope=0.2)\n",
      "    (output_layer): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (output_activation): Sigmoid()\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "discriminator = Discriminator()\n",
    "\n",
    "print(discriminator.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: model.0_layer.weight & # of parameters: 401408 \n",
      "Layer: model.0_layer.bias & # of parameters: 512 \n",
      "Layer: model.1_layer.weight & # of parameters: 131072 \n",
      "Layer: model.1_layer.bias & # of parameters: 256 \n",
      "Layer: model.output_layer.weight & # of parameters: 256 \n",
      "Layer: model.output_layer.bias & # of parameters: 1 \n",
      "\n",
      "TOTAL NUMBER OF PARAMETERS OF DISCRIMINATOR IS 533505 \n"
     ]
    }
   ],
   "source": [
    "# Total number of parameters of Discriminator\n",
    "\n",
    "total_parameters = 0\n",
    "for layer, params in discriminator.named_parameters():\n",
    "    total_parameters+=params.numel()\n",
    "    print(\"Layer: {} & # of parameters: {} \".format(layer, params.numel()))\n",
    "    \n",
    "\n",
    "print(\"\\nTotal number of parameters of discriminator is {} \".format(total_parameters).upper()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "# Check the GPU in MAC\n",
    "device = (\n",
    "    torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the GPU\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "\n",
    "loss_function = nn.BCELoss()\n",
    "learning_rate = 0.0001\n",
    "\n",
    "optimizer_generator = optim.Adam(\n",
    "    generator.parameters(), lr=learning_rate, betas=(0.5, 0.999)\n",
    ")\n",
    "optimizer_discriminator = optim.Adam(\n",
    "    discriminator.parameters(), lr=learning_rate, betas= (0.5, 0.999)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator data using noise #  torch.Size([64, 1, 28, 28])\n",
      "Discriminator shape #  torch.Size([4096, 1])\n"
     ]
    }
   ],
   "source": [
    "# check whether Generator and Discriminator works or not !\n",
    "batch_size = 64\n",
    "latent_space = 100\n",
    "\n",
    "# Generator\n",
    "noise_data = torch.randn(batch_size, latent_space)\n",
    "print(\"Generator data using noise # \", generator(noise_data.to(device)).shape)\n",
    "\n",
    "# Discriminator\n",
    "dataset = torch.randn(batch_size, 64, 28, 28)\n",
    "print(\"Discriminator shape # \", discriminator(dataset.to(device)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [0/938], d_loss: 1.3969, g_loss: 0.6777\n",
      "Epoch [1/100], Step [100/938], d_loss: 1.3616, g_loss: 0.7243\n",
      "Epoch [1/100], Step [200/938], d_loss: 1.3294, g_loss: 0.7234\n",
      "Epoch [1/100], Step [300/938], d_loss: 1.2967, g_loss: 0.8190\n",
      "Epoch [1/100], Step [400/938], d_loss: 1.1841, g_loss: 0.8169\n",
      "Epoch [1/100], Step [500/938], d_loss: 1.4153, g_loss: 0.7578\n",
      "Epoch [1/100], Step [600/938], d_loss: 1.2028, g_loss: 1.0552\n",
      "Epoch [1/100], Step [700/938], d_loss: 1.2939, g_loss: 0.9768\n",
      "Epoch [1/100], Step [800/938], d_loss: 1.4241, g_loss: 0.8936\n",
      "Epoch [1/100], Step [900/938], d_loss: 1.1223, g_loss: 1.0500\n",
      "Epoch [1/100] Completed\n",
      "[==============] Average d_loss: 1.3019 - Average g_loss: 0.8511\n",
      "Epoch [2/100], Step [0/938], d_loss: 1.4426, g_loss: 1.1947\n",
      "Epoch [2/100], Step [100/938], d_loss: 1.2612, g_loss: 1.1199\n",
      "Epoch [2/100], Step [200/938], d_loss: 1.3420, g_loss: 0.8174\n",
      "Epoch [2/100], Step [300/938], d_loss: 1.2889, g_loss: 1.1074\n",
      "Epoch [2/100], Step [400/938], d_loss: 1.3697, g_loss: 0.9870\n",
      "Epoch [2/100], Step [500/938], d_loss: 1.2330, g_loss: 1.1847\n",
      "Epoch [2/100], Step [600/938], d_loss: 1.3151, g_loss: 1.3084\n",
      "Epoch [2/100], Step [700/938], d_loss: 1.0091, g_loss: 1.1261\n",
      "Epoch [2/100], Step [800/938], d_loss: 1.6420, g_loss: 1.1568\n",
      "Epoch [2/100], Step [900/938], d_loss: 0.9947, g_loss: 1.2035\n",
      "Epoch [2/100] Completed\n",
      "[==============] Average d_loss: 1.2440 - Average g_loss: 1.0886\n",
      "Epoch [3/100], Step [0/938], d_loss: 1.2712, g_loss: 0.9577\n",
      "Epoch [3/100], Step [100/938], d_loss: 1.1980, g_loss: 1.0901\n",
      "Epoch [3/100], Step [200/938], d_loss: 0.8609, g_loss: 1.2587\n",
      "Epoch [3/100], Step [300/938], d_loss: 1.2512, g_loss: 1.5733\n",
      "Epoch [3/100], Step [400/938], d_loss: 1.0280, g_loss: 1.5471\n",
      "Epoch [3/100], Step [500/938], d_loss: 1.1445, g_loss: 1.2667\n",
      "Epoch [3/100], Step [600/938], d_loss: 1.0680, g_loss: 1.3318\n",
      "Epoch [3/100], Step [700/938], d_loss: 0.9667, g_loss: 1.3356\n",
      "Epoch [3/100], Step [800/938], d_loss: 1.2400, g_loss: 1.7293\n",
      "Epoch [3/100], Step [900/938], d_loss: 1.0791, g_loss: 1.3552\n",
      "Epoch [3/100] Completed\n",
      "[==============] Average d_loss: 1.0978 - Average g_loss: 1.3729\n",
      "Epoch [4/100], Step [0/938], d_loss: 1.3632, g_loss: 1.7390\n",
      "Epoch [4/100], Step [100/938], d_loss: 1.3184, g_loss: 1.5512\n",
      "Epoch [4/100], Step [200/938], d_loss: 1.1769, g_loss: 1.6606\n",
      "Epoch [4/100], Step [300/938], d_loss: 0.8959, g_loss: 1.7995\n",
      "Epoch [4/100], Step [400/938], d_loss: 1.0758, g_loss: 1.7636\n",
      "Epoch [4/100], Step [500/938], d_loss: 0.8346, g_loss: 1.8515\n",
      "Epoch [4/100], Step [600/938], d_loss: 0.7591, g_loss: 1.8951\n",
      "Epoch [4/100], Step [700/938], d_loss: 0.8580, g_loss: 1.8147\n",
      "Epoch [4/100], Step [800/938], d_loss: 1.1411, g_loss: 1.8160\n",
      "Epoch [4/100], Step [900/938], d_loss: 0.7665, g_loss: 1.9868\n",
      "Epoch [4/100] Completed\n",
      "[==============] Average d_loss: 0.9563 - Average g_loss: 1.8168\n",
      "Epoch [5/100], Step [0/938], d_loss: 0.8191, g_loss: 2.3228\n",
      "Epoch [5/100], Step [100/938], d_loss: 1.0607, g_loss: 1.9493\n",
      "Epoch [5/100], Step [200/938], d_loss: 0.7278, g_loss: 2.5659\n",
      "Epoch [5/100], Step [300/938], d_loss: 0.9776, g_loss: 2.2805\n",
      "Epoch [5/100], Step [400/938], d_loss: 0.8442, g_loss: 2.0353\n",
      "Epoch [5/100], Step [500/938], d_loss: 0.7091, g_loss: 2.4702\n",
      "Epoch [5/100], Step [600/938], d_loss: 0.6087, g_loss: 2.1415\n",
      "Epoch [5/100], Step [700/938], d_loss: 0.8042, g_loss: 2.3082\n",
      "Epoch [5/100], Step [800/938], d_loss: 0.6901, g_loss: 2.3227\n",
      "Epoch [5/100], Step [900/938], d_loss: 0.6526, g_loss: 2.6385\n",
      "Epoch [5/100] Completed\n",
      "[==============] Average d_loss: 0.7574 - Average g_loss: 2.3635\n",
      "Epoch [6/100], Step [0/938], d_loss: 0.5154, g_loss: 2.6359\n",
      "Epoch [6/100], Step [100/938], d_loss: 0.7703, g_loss: 2.7594\n",
      "Epoch [6/100], Step [200/938], d_loss: 0.6056, g_loss: 2.8933\n",
      "Epoch [6/100], Step [300/938], d_loss: 0.9000, g_loss: 2.9090\n",
      "Epoch [6/100], Step [400/938], d_loss: 0.6302, g_loss: 3.0126\n",
      "Epoch [6/100], Step [500/938], d_loss: 0.7518, g_loss: 2.5484\n",
      "Epoch [6/100], Step [600/938], d_loss: 0.6375, g_loss: 2.6979\n",
      "Epoch [6/100], Step [700/938], d_loss: 0.6709, g_loss: 2.5307\n",
      "Epoch [6/100], Step [800/938], d_loss: 0.7193, g_loss: 2.7958\n",
      "Epoch [6/100], Step [900/938], d_loss: 0.6376, g_loss: 2.8282\n",
      "Epoch [6/100] Completed\n",
      "[==============] Average d_loss: 0.6226 - Average g_loss: 2.8157\n",
      "Epoch [7/100], Step [0/938], d_loss: 0.5118, g_loss: 3.0299\n",
      "Epoch [7/100], Step [100/938], d_loss: 0.6837, g_loss: 3.0544\n",
      "Epoch [7/100], Step [200/938], d_loss: 0.5858, g_loss: 2.9977\n",
      "Epoch [7/100], Step [300/938], d_loss: 0.5160, g_loss: 2.5365\n",
      "Epoch [7/100], Step [400/938], d_loss: 0.7971, g_loss: 2.4227\n",
      "Epoch [7/100], Step [500/938], d_loss: 0.4390, g_loss: 3.2431\n",
      "Epoch [7/100], Step [600/938], d_loss: 0.5045, g_loss: 3.3162\n",
      "Epoch [7/100], Step [700/938], d_loss: 0.6415, g_loss: 2.7215\n",
      "Epoch [7/100], Step [800/938], d_loss: 0.5751, g_loss: 3.2855\n",
      "Epoch [7/100], Step [900/938], d_loss: 0.5743, g_loss: 3.3236\n",
      "Epoch [7/100] Completed\n",
      "[==============] Average d_loss: 0.5396 - Average g_loss: 3.0899\n",
      "Epoch [8/100], Step [0/938], d_loss: 0.6109, g_loss: 3.6936\n",
      "Epoch [8/100], Step [100/938], d_loss: 0.5962, g_loss: 3.2460\n",
      "Epoch [8/100], Step [200/938], d_loss: 0.5735, g_loss: 3.2506\n",
      "Epoch [8/100], Step [300/938], d_loss: 0.3011, g_loss: 3.5316\n",
      "Epoch [8/100], Step [400/938], d_loss: 0.3411, g_loss: 3.0016\n",
      "Epoch [8/100], Step [500/938], d_loss: 0.2852, g_loss: 4.0176\n",
      "Epoch [8/100], Step [600/938], d_loss: 0.4823, g_loss: 3.2335\n",
      "Epoch [8/100], Step [700/938], d_loss: 0.5058, g_loss: 3.3629\n",
      "Epoch [8/100], Step [800/938], d_loss: 0.4304, g_loss: 3.7134\n",
      "Epoch [8/100], Step [900/938], d_loss: 0.6031, g_loss: 3.9480\n",
      "Epoch [8/100] Completed\n",
      "[==============] Average d_loss: 0.4909 - Average g_loss: 3.3686\n",
      "Epoch [9/100], Step [0/938], d_loss: 0.3747, g_loss: 3.6756\n",
      "Epoch [9/100], Step [100/938], d_loss: 0.5931, g_loss: 3.1329\n",
      "Epoch [9/100], Step [200/938], d_loss: 0.3368, g_loss: 3.6950\n",
      "Epoch [9/100], Step [300/938], d_loss: 0.5293, g_loss: 3.5756\n",
      "Epoch [9/100], Step [400/938], d_loss: 0.5083, g_loss: 3.4171\n",
      "Epoch [9/100], Step [500/938], d_loss: 0.3647, g_loss: 3.4903\n",
      "Epoch [9/100], Step [600/938], d_loss: 0.4233, g_loss: 3.5115\n",
      "Epoch [9/100], Step [700/938], d_loss: 0.3971, g_loss: 3.2337\n",
      "Epoch [9/100], Step [800/938], d_loss: 0.6622, g_loss: 2.5671\n",
      "Epoch [9/100], Step [900/938], d_loss: 0.4530, g_loss: 4.0863\n",
      "Epoch [9/100] Completed\n",
      "[==============] Average d_loss: 0.4963 - Average g_loss: 3.3327\n",
      "Epoch [10/100], Step [0/938], d_loss: 0.3546, g_loss: 3.5869\n",
      "Epoch [10/100], Step [100/938], d_loss: 0.5711, g_loss: 3.1593\n",
      "Epoch [10/100], Step [200/938], d_loss: 0.5491, g_loss: 3.4015\n",
      "Epoch [10/100], Step [300/938], d_loss: 0.5225, g_loss: 2.8976\n",
      "Epoch [10/100], Step [400/938], d_loss: 0.5977, g_loss: 2.6820\n",
      "Epoch [10/100], Step [500/938], d_loss: 0.7189, g_loss: 2.8042\n",
      "Epoch [10/100], Step [600/938], d_loss: 0.5239, g_loss: 3.2913\n",
      "Epoch [10/100], Step [700/938], d_loss: 0.6071, g_loss: 2.9192\n",
      "Epoch [10/100], Step [800/938], d_loss: 0.5496, g_loss: 2.9407\n",
      "Epoch [10/100], Step [900/938], d_loss: 0.6341, g_loss: 2.5948\n",
      "Epoch [10/100] Completed\n",
      "[==============] Average d_loss: 0.5507 - Average g_loss: 3.1408\n",
      "Epoch [11/100], Step [0/938], d_loss: 0.6361, g_loss: 3.1146\n",
      "Epoch [11/100], Step [100/938], d_loss: 0.5887, g_loss: 3.2439\n",
      "Epoch [11/100], Step [200/938], d_loss: 0.3400, g_loss: 2.9922\n",
      "Epoch [11/100], Step [300/938], d_loss: 0.5612, g_loss: 3.1019\n",
      "Epoch [11/100], Step [400/938], d_loss: 0.6084, g_loss: 2.9941\n",
      "Epoch [11/100], Step [500/938], d_loss: 0.4611, g_loss: 3.4903\n",
      "Epoch [11/100], Step [600/938], d_loss: 0.5052, g_loss: 3.4534\n",
      "Epoch [11/100], Step [700/938], d_loss: 0.5055, g_loss: 3.1136\n",
      "Epoch [11/100], Step [800/938], d_loss: 0.6655, g_loss: 3.7720\n",
      "Epoch [11/100], Step [900/938], d_loss: 0.4010, g_loss: 2.8307\n",
      "Epoch [11/100] Completed\n",
      "[==============] Average d_loss: 0.5428 - Average g_loss: 3.0802\n",
      "Epoch [12/100], Step [0/938], d_loss: 0.5224, g_loss: 2.8614\n",
      "Epoch [12/100], Step [100/938], d_loss: 0.4682, g_loss: 3.7701\n",
      "Epoch [12/100], Step [200/938], d_loss: 0.6248, g_loss: 2.8057\n",
      "Epoch [12/100], Step [300/938], d_loss: 0.3785, g_loss: 2.9712\n",
      "Epoch [12/100], Step [400/938], d_loss: 0.4853, g_loss: 3.2840\n",
      "Epoch [12/100], Step [500/938], d_loss: 0.7088, g_loss: 3.0957\n",
      "Epoch [12/100], Step [600/938], d_loss: 0.5649, g_loss: 2.8363\n",
      "Epoch [12/100], Step [700/938], d_loss: 0.6901, g_loss: 2.6545\n",
      "Epoch [12/100], Step [800/938], d_loss: 0.5161, g_loss: 2.8791\n",
      "Epoch [12/100], Step [900/938], d_loss: 0.4941, g_loss: 2.9939\n",
      "Epoch [12/100] Completed\n",
      "[==============] Average d_loss: 0.5443 - Average g_loss: 3.1277\n",
      "Epoch [13/100], Step [0/938], d_loss: 0.6242, g_loss: 2.7408\n",
      "Epoch [13/100], Step [100/938], d_loss: 0.5008, g_loss: 2.9293\n",
      "Epoch [13/100], Step [200/938], d_loss: 0.6416, g_loss: 2.3638\n",
      "Epoch [13/100], Step [300/938], d_loss: 0.6646, g_loss: 2.4129\n",
      "Epoch [13/100], Step [400/938], d_loss: 0.6198, g_loss: 2.5318\n",
      "Epoch [13/100], Step [500/938], d_loss: 0.5387, g_loss: 2.4692\n",
      "Epoch [13/100], Step [600/938], d_loss: 0.6947, g_loss: 3.3112\n",
      "Epoch [13/100], Step [700/938], d_loss: 0.4291, g_loss: 3.3763\n",
      "Epoch [13/100], Step [800/938], d_loss: 0.8245, g_loss: 3.0400\n",
      "Epoch [13/100], Step [900/938], d_loss: 0.6966, g_loss: 2.7229\n",
      "Epoch [13/100] Completed\n",
      "[==============] Average d_loss: 0.5796 - Average g_loss: 2.9510\n",
      "Epoch [14/100], Step [0/938], d_loss: 0.5437, g_loss: 2.5960\n",
      "Epoch [14/100], Step [100/938], d_loss: 0.8301, g_loss: 3.1150\n",
      "Epoch [14/100], Step [200/938], d_loss: 0.4034, g_loss: 2.5964\n",
      "Epoch [14/100], Step [300/938], d_loss: 0.6982, g_loss: 2.5084\n",
      "Epoch [14/100], Step [400/938], d_loss: 0.6238, g_loss: 2.5668\n",
      "Epoch [14/100], Step [500/938], d_loss: 0.6808, g_loss: 2.7409\n",
      "Epoch [14/100], Step [600/938], d_loss: 0.7183, g_loss: 3.4088\n",
      "Epoch [14/100], Step [700/938], d_loss: 0.5357, g_loss: 3.3703\n",
      "Epoch [14/100], Step [800/938], d_loss: 0.7565, g_loss: 2.3377\n",
      "Epoch [14/100], Step [900/938], d_loss: 0.7559, g_loss: 2.7391\n",
      "Epoch [14/100] Completed\n",
      "[==============] Average d_loss: 0.5829 - Average g_loss: 2.9448\n",
      "Epoch [15/100], Step [0/938], d_loss: 0.4999, g_loss: 2.6685\n",
      "Epoch [15/100], Step [100/938], d_loss: 0.5978, g_loss: 2.7805\n",
      "Epoch [15/100], Step [200/938], d_loss: 0.7328, g_loss: 2.6602\n",
      "Epoch [15/100], Step [300/938], d_loss: 1.0049, g_loss: 3.0253\n",
      "Epoch [15/100], Step [400/938], d_loss: 0.4545, g_loss: 3.2476\n",
      "Epoch [15/100], Step [500/938], d_loss: 0.5917, g_loss: 2.5606\n",
      "Epoch [15/100], Step [600/938], d_loss: 0.6091, g_loss: 2.5230\n",
      "Epoch [15/100], Step [700/938], d_loss: 0.5665, g_loss: 2.8876\n",
      "Epoch [15/100], Step [800/938], d_loss: 0.5577, g_loss: 2.6160\n",
      "Epoch [15/100], Step [900/938], d_loss: 0.6381, g_loss: 3.2132\n",
      "Epoch [15/100] Completed\n",
      "[==============] Average d_loss: 0.5856 - Average g_loss: 2.9591\n",
      "Epoch [16/100], Step [0/938], d_loss: 0.5423, g_loss: 2.9757\n",
      "Epoch [16/100], Step [100/938], d_loss: 0.5354, g_loss: 3.2430\n",
      "Epoch [16/100], Step [200/938], d_loss: 0.5656, g_loss: 3.1040\n",
      "Epoch [16/100], Step [300/938], d_loss: 0.6681, g_loss: 3.0020\n",
      "Epoch [16/100], Step [400/938], d_loss: 0.4530, g_loss: 2.6956\n",
      "Epoch [16/100], Step [500/938], d_loss: 0.5936, g_loss: 3.3138\n",
      "Epoch [16/100], Step [600/938], d_loss: 0.6729, g_loss: 2.6961\n",
      "Epoch [16/100], Step [700/938], d_loss: 0.5979, g_loss: 2.7369\n",
      "Epoch [16/100], Step [800/938], d_loss: 0.4472, g_loss: 2.7858\n",
      "Epoch [16/100], Step [900/938], d_loss: 0.6637, g_loss: 2.0545\n",
      "Epoch [16/100] Completed\n",
      "[==============] Average d_loss: 0.5844 - Average g_loss: 2.8219\n",
      "Epoch [17/100], Step [0/938], d_loss: 0.7029, g_loss: 2.8103\n",
      "Epoch [17/100], Step [100/938], d_loss: 0.7465, g_loss: 2.4688\n",
      "Epoch [17/100], Step [200/938], d_loss: 0.3788, g_loss: 2.8189\n",
      "Epoch [17/100], Step [300/938], d_loss: 0.5740, g_loss: 2.8755\n",
      "Epoch [17/100], Step [400/938], d_loss: 0.6712, g_loss: 2.5558\n",
      "Epoch [17/100], Step [500/938], d_loss: 0.5527, g_loss: 3.2261\n",
      "Epoch [17/100], Step [600/938], d_loss: 0.6837, g_loss: 2.6497\n",
      "Epoch [17/100], Step [700/938], d_loss: 0.4200, g_loss: 3.0925\n",
      "Epoch [17/100], Step [800/938], d_loss: 0.7495, g_loss: 2.6107\n",
      "Epoch [17/100], Step [900/938], d_loss: 0.6436, g_loss: 3.1283\n",
      "Epoch [17/100] Completed\n",
      "[==============] Average d_loss: 0.5881 - Average g_loss: 2.8677\n",
      "Epoch [18/100], Step [0/938], d_loss: 0.6428, g_loss: 3.8282\n",
      "Epoch [18/100], Step [100/938], d_loss: 0.6889, g_loss: 3.0050\n",
      "Epoch [18/100], Step [200/938], d_loss: 0.6420, g_loss: 2.4258\n",
      "Epoch [18/100], Step [300/938], d_loss: 0.4753, g_loss: 3.3753\n",
      "Epoch [18/100], Step [400/938], d_loss: 0.5987, g_loss: 3.2217\n",
      "Epoch [18/100], Step [500/938], d_loss: 0.3926, g_loss: 3.2109\n",
      "Epoch [18/100], Step [600/938], d_loss: 0.4647, g_loss: 3.6044\n",
      "Epoch [18/100], Step [700/938], d_loss: 0.5290, g_loss: 3.7779\n",
      "Epoch [18/100], Step [800/938], d_loss: 0.4410, g_loss: 2.6337\n",
      "Epoch [18/100], Step [900/938], d_loss: 0.7632, g_loss: 2.5032\n",
      "Epoch [18/100] Completed\n",
      "[==============] Average d_loss: 0.5669 - Average g_loss: 2.9355\n",
      "Epoch [19/100], Step [0/938], d_loss: 0.4878, g_loss: 2.6866\n",
      "Epoch [19/100], Step [100/938], d_loss: 0.5672, g_loss: 3.5842\n",
      "Epoch [19/100], Step [200/938], d_loss: 0.5564, g_loss: 3.6566\n",
      "Epoch [19/100], Step [300/938], d_loss: 0.6436, g_loss: 2.0172\n",
      "Epoch [19/100], Step [400/938], d_loss: 0.5326, g_loss: 2.4447\n",
      "Epoch [19/100], Step [500/938], d_loss: 0.5948, g_loss: 2.8350\n",
      "Epoch [19/100], Step [600/938], d_loss: 0.4908, g_loss: 2.8835\n",
      "Epoch [19/100], Step [700/938], d_loss: 0.4897, g_loss: 2.9236\n",
      "Epoch [19/100], Step [800/938], d_loss: 0.7921, g_loss: 3.5936\n",
      "Epoch [19/100], Step [900/938], d_loss: 0.5104, g_loss: 3.2767\n",
      "Epoch [19/100] Completed\n",
      "[==============] Average d_loss: 0.5490 - Average g_loss: 2.9648\n",
      "Epoch [20/100], Step [0/938], d_loss: 0.4555, g_loss: 3.0204\n",
      "Epoch [20/100], Step [100/938], d_loss: 0.5835, g_loss: 2.8304\n",
      "Epoch [20/100], Step [200/938], d_loss: 0.7989, g_loss: 2.1278\n",
      "Epoch [20/100], Step [300/938], d_loss: 0.4677, g_loss: 2.5809\n",
      "Epoch [20/100], Step [400/938], d_loss: 0.4921, g_loss: 2.9633\n",
      "Epoch [20/100], Step [500/938], d_loss: 0.4967, g_loss: 2.7254\n",
      "Epoch [20/100], Step [600/938], d_loss: 0.4613, g_loss: 3.0219\n",
      "Epoch [20/100], Step [700/938], d_loss: 0.4397, g_loss: 2.7461\n",
      "Epoch [20/100], Step [800/938], d_loss: 0.5004, g_loss: 3.0259\n",
      "Epoch [20/100], Step [900/938], d_loss: 0.4966, g_loss: 2.9770\n",
      "Epoch [20/100] Completed\n",
      "[==============] Average d_loss: 0.5443 - Average g_loss: 2.9807\n",
      "Epoch [21/100], Step [0/938], d_loss: 0.4900, g_loss: 2.7754\n",
      "Epoch [21/100], Step [100/938], d_loss: 0.3908, g_loss: 3.3028\n",
      "Epoch [21/100], Step [200/938], d_loss: 0.4054, g_loss: 2.7415\n",
      "Epoch [21/100], Step [300/938], d_loss: 0.5405, g_loss: 3.1003\n",
      "Epoch [21/100], Step [400/938], d_loss: 0.3490, g_loss: 3.1506\n",
      "Epoch [21/100], Step [500/938], d_loss: 0.6554, g_loss: 2.6210\n",
      "Epoch [21/100], Step [600/938], d_loss: 0.4713, g_loss: 3.0229\n",
      "Epoch [21/100], Step [700/938], d_loss: 0.6033, g_loss: 2.5015\n",
      "Epoch [21/100], Step [800/938], d_loss: 0.6148, g_loss: 2.8402\n",
      "Epoch [21/100], Step [900/938], d_loss: 0.4177, g_loss: 3.5180\n",
      "Epoch [21/100] Completed\n",
      "[==============] Average d_loss: 0.5333 - Average g_loss: 3.0412\n",
      "Epoch [22/100], Step [0/938], d_loss: 0.6188, g_loss: 2.6034\n",
      "Epoch [22/100], Step [100/938], d_loss: 0.4945, g_loss: 2.7492\n",
      "Epoch [22/100], Step [200/938], d_loss: 0.5229, g_loss: 2.7395\n",
      "Epoch [22/100], Step [300/938], d_loss: 0.4919, g_loss: 3.7335\n",
      "Epoch [22/100], Step [400/938], d_loss: 0.4152, g_loss: 2.8767\n",
      "Epoch [22/100], Step [500/938], d_loss: 0.4011, g_loss: 2.7633\n",
      "Epoch [22/100], Step [600/938], d_loss: 0.6896, g_loss: 2.6452\n",
      "Epoch [22/100], Step [700/938], d_loss: 0.5084, g_loss: 3.3421\n",
      "Epoch [22/100], Step [800/938], d_loss: 0.4133, g_loss: 3.2500\n",
      "Epoch [22/100], Step [900/938], d_loss: 0.5316, g_loss: 2.8101\n",
      "Epoch [22/100] Completed\n",
      "[==============] Average d_loss: 0.5093 - Average g_loss: 3.0605\n",
      "Epoch [23/100], Step [0/938], d_loss: 0.6163, g_loss: 3.0013\n",
      "Epoch [23/100], Step [100/938], d_loss: 0.3753, g_loss: 3.5039\n",
      "Epoch [23/100], Step [200/938], d_loss: 0.4264, g_loss: 2.6770\n",
      "Epoch [23/100], Step [300/938], d_loss: 0.3808, g_loss: 3.6011\n",
      "Epoch [23/100], Step [400/938], d_loss: 0.5550, g_loss: 3.1174\n",
      "Epoch [23/100], Step [500/938], d_loss: 0.3619, g_loss: 3.5093\n",
      "Epoch [23/100], Step [600/938], d_loss: 0.4482, g_loss: 3.5107\n",
      "Epoch [23/100], Step [700/938], d_loss: 0.6129, g_loss: 2.8301\n",
      "Epoch [23/100], Step [800/938], d_loss: 0.5884, g_loss: 2.8600\n",
      "Epoch [23/100], Step [900/938], d_loss: 0.5357, g_loss: 3.0690\n",
      "Epoch [23/100] Completed\n",
      "[==============] Average d_loss: 0.4765 - Average g_loss: 3.1716\n",
      "Epoch [24/100], Step [0/938], d_loss: 0.6405, g_loss: 2.8527\n",
      "Epoch [24/100], Step [100/938], d_loss: 0.4422, g_loss: 2.8473\n",
      "Epoch [24/100], Step [200/938], d_loss: 0.6388, g_loss: 2.7570\n",
      "Epoch [24/100], Step [300/938], d_loss: 0.4052, g_loss: 3.2341\n",
      "Epoch [24/100], Step [400/938], d_loss: 0.3656, g_loss: 2.8878\n",
      "Epoch [24/100], Step [500/938], d_loss: 0.7184, g_loss: 3.8069\n",
      "Epoch [24/100], Step [600/938], d_loss: 0.4400, g_loss: 3.6930\n",
      "Epoch [24/100], Step [700/938], d_loss: 0.3695, g_loss: 3.5937\n",
      "Epoch [24/100], Step [800/938], d_loss: 0.6147, g_loss: 3.0571\n",
      "Epoch [24/100], Step [900/938], d_loss: 0.4119, g_loss: 3.0560\n",
      "Epoch [24/100] Completed\n",
      "[==============] Average d_loss: 0.4763 - Average g_loss: 3.2471\n",
      "Epoch [25/100], Step [0/938], d_loss: 0.8206, g_loss: 4.1179\n",
      "Epoch [25/100], Step [100/938], d_loss: 0.4842, g_loss: 3.0715\n",
      "Epoch [25/100], Step [200/938], d_loss: 0.4557, g_loss: 3.0881\n",
      "Epoch [25/100], Step [300/938], d_loss: 0.4093, g_loss: 3.2917\n",
      "Epoch [25/100], Step [400/938], d_loss: 0.4694, g_loss: 2.8482\n",
      "Epoch [25/100], Step [500/938], d_loss: 0.6198, g_loss: 2.5092\n",
      "Epoch [25/100], Step [600/938], d_loss: 0.5845, g_loss: 4.0554\n",
      "Epoch [25/100], Step [700/938], d_loss: 0.4241, g_loss: 3.6028\n",
      "Epoch [25/100], Step [800/938], d_loss: 0.4734, g_loss: 4.1502\n",
      "Epoch [25/100], Step [900/938], d_loss: 0.5049, g_loss: 3.1620\n",
      "Epoch [25/100] Completed\n",
      "[==============] Average d_loss: 0.4790 - Average g_loss: 3.1902\n",
      "Epoch [26/100], Step [0/938], d_loss: 0.5286, g_loss: 3.0980\n",
      "Epoch [26/100], Step [100/938], d_loss: 0.5309, g_loss: 4.1904\n",
      "Epoch [26/100], Step [200/938], d_loss: 0.5094, g_loss: 3.1870\n",
      "Epoch [26/100], Step [300/938], d_loss: 0.4098, g_loss: 3.5829\n",
      "Epoch [26/100], Step [400/938], d_loss: 0.5735, g_loss: 3.1117\n",
      "Epoch [26/100], Step [500/938], d_loss: 0.3507, g_loss: 3.6241\n",
      "Epoch [26/100], Step [600/938], d_loss: 0.5096, g_loss: 3.3899\n",
      "Epoch [26/100], Step [700/938], d_loss: 0.3934, g_loss: 3.1041\n",
      "Epoch [26/100], Step [800/938], d_loss: 0.4962, g_loss: 3.6108\n",
      "Epoch [26/100], Step [900/938], d_loss: 0.5700, g_loss: 2.9632\n",
      "Epoch [26/100] Completed\n",
      "[==============] Average d_loss: 0.4595 - Average g_loss: 3.1919\n",
      "Epoch [27/100], Step [0/938], d_loss: 0.2952, g_loss: 2.9018\n",
      "Epoch [27/100], Step [100/938], d_loss: 0.4039, g_loss: 3.0486\n",
      "Epoch [27/100], Step [200/938], d_loss: 0.2814, g_loss: 3.0782\n",
      "Epoch [27/100], Step [300/938], d_loss: 0.4862, g_loss: 3.6952\n",
      "Epoch [27/100], Step [400/938], d_loss: 0.2082, g_loss: 4.4833\n",
      "Epoch [27/100], Step [500/938], d_loss: 0.5490, g_loss: 2.6846\n",
      "Epoch [27/100], Step [600/938], d_loss: 0.2804, g_loss: 3.4999\n",
      "Epoch [27/100], Step [700/938], d_loss: 0.3220, g_loss: 3.5143\n",
      "Epoch [27/100], Step [800/938], d_loss: 0.3426, g_loss: 3.4989\n",
      "Epoch [27/100], Step [900/938], d_loss: 0.3385, g_loss: 3.2822\n",
      "Epoch [27/100] Completed\n",
      "[==============] Average d_loss: 0.4241 - Average g_loss: 3.3846\n",
      "Epoch [28/100], Step [0/938], d_loss: 0.4632, g_loss: 2.8865\n",
      "Epoch [28/100], Step [100/938], d_loss: 0.3006, g_loss: 4.1294\n",
      "Epoch [28/100], Step [200/938], d_loss: 0.3616, g_loss: 2.8674\n",
      "Epoch [28/100], Step [300/938], d_loss: 0.4403, g_loss: 3.5034\n",
      "Epoch [28/100], Step [400/938], d_loss: 0.4896, g_loss: 3.1008\n",
      "Epoch [28/100], Step [500/938], d_loss: 0.5128, g_loss: 3.6018\n",
      "Epoch [28/100], Step [600/938], d_loss: 0.4500, g_loss: 2.5774\n",
      "Epoch [28/100], Step [700/938], d_loss: 0.4124, g_loss: 3.9388\n",
      "Epoch [28/100], Step [800/938], d_loss: 0.4501, g_loss: 2.9334\n",
      "Epoch [28/100], Step [900/938], d_loss: 0.5598, g_loss: 3.2966\n",
      "Epoch [28/100] Completed\n",
      "[==============] Average d_loss: 0.4365 - Average g_loss: 3.4261\n",
      "Epoch [29/100], Step [0/938], d_loss: 0.4501, g_loss: 3.4570\n",
      "Epoch [29/100], Step [100/938], d_loss: 0.3846, g_loss: 3.7035\n",
      "Epoch [29/100], Step [200/938], d_loss: 0.5510, g_loss: 2.5808\n",
      "Epoch [29/100], Step [300/938], d_loss: 0.3753, g_loss: 3.5436\n",
      "Epoch [29/100], Step [400/938], d_loss: 0.3166, g_loss: 3.8820\n",
      "Epoch [29/100], Step [500/938], d_loss: 0.3210, g_loss: 3.5771\n",
      "Epoch [29/100], Step [600/938], d_loss: 0.6795, g_loss: 4.1951\n",
      "Epoch [29/100], Step [700/938], d_loss: 0.4051, g_loss: 4.0907\n",
      "Epoch [29/100], Step [800/938], d_loss: 0.2681, g_loss: 3.7672\n",
      "Epoch [29/100], Step [900/938], d_loss: 0.3494, g_loss: 3.9302\n",
      "Epoch [29/100] Completed\n",
      "[==============] Average d_loss: 0.4093 - Average g_loss: 3.4339\n",
      "Epoch [30/100], Step [0/938], d_loss: 0.5713, g_loss: 3.8702\n",
      "Epoch [30/100], Step [100/938], d_loss: 0.3626, g_loss: 3.5444\n",
      "Epoch [30/100], Step [200/938], d_loss: 0.2962, g_loss: 3.1692\n",
      "Epoch [30/100], Step [300/938], d_loss: 0.4233, g_loss: 2.8277\n",
      "Epoch [30/100], Step [400/938], d_loss: 0.3701, g_loss: 3.8695\n",
      "Epoch [30/100], Step [500/938], d_loss: 0.2151, g_loss: 3.1788\n",
      "Epoch [30/100], Step [600/938], d_loss: 0.3883, g_loss: 3.9694\n",
      "Epoch [30/100], Step [700/938], d_loss: 0.3613, g_loss: 2.9617\n",
      "Epoch [30/100], Step [800/938], d_loss: 0.3413, g_loss: 3.6350\n",
      "Epoch [30/100], Step [900/938], d_loss: 0.5838, g_loss: 5.7429\n",
      "Epoch [30/100] Completed\n",
      "[==============] Average d_loss: 0.4043 - Average g_loss: 3.5608\n",
      "Epoch [31/100], Step [0/938], d_loss: 0.3345, g_loss: 3.5276\n",
      "Epoch [31/100], Step [100/938], d_loss: 0.3426, g_loss: 3.7952\n",
      "Epoch [31/100], Step [200/938], d_loss: 0.5529, g_loss: 2.8719\n",
      "Epoch [31/100], Step [300/938], d_loss: 0.3863, g_loss: 4.0278\n",
      "Epoch [31/100], Step [400/938], d_loss: 0.3443, g_loss: 3.1451\n",
      "Epoch [31/100], Step [500/938], d_loss: 0.3630, g_loss: 3.4380\n",
      "Epoch [31/100], Step [600/938], d_loss: 0.4279, g_loss: 3.2349\n",
      "Epoch [31/100], Step [700/938], d_loss: 0.3156, g_loss: 4.5249\n",
      "Epoch [31/100], Step [800/938], d_loss: 0.4145, g_loss: 2.6790\n",
      "Epoch [31/100], Step [900/938], d_loss: 0.2777, g_loss: 3.2890\n",
      "Epoch [31/100] Completed\n",
      "[==============] Average d_loss: 0.3921 - Average g_loss: 3.5556\n",
      "Epoch [32/100], Step [0/938], d_loss: 0.5163, g_loss: 2.5101\n",
      "Epoch [32/100], Step [100/938], d_loss: 0.4132, g_loss: 4.0463\n",
      "Epoch [32/100], Step [200/938], d_loss: 0.2916, g_loss: 3.2192\n",
      "Epoch [32/100], Step [300/938], d_loss: 0.4948, g_loss: 3.1918\n",
      "Epoch [32/100], Step [400/938], d_loss: 0.4035, g_loss: 4.3356\n",
      "Epoch [32/100], Step [500/938], d_loss: 0.4329, g_loss: 3.3821\n",
      "Epoch [32/100], Step [600/938], d_loss: 0.4839, g_loss: 4.2505\n",
      "Epoch [32/100], Step [700/938], d_loss: 0.2976, g_loss: 3.4359\n",
      "Epoch [32/100], Step [800/938], d_loss: 0.4806, g_loss: 5.7219\n",
      "Epoch [32/100], Step [900/938], d_loss: 0.3947, g_loss: 3.9073\n",
      "Epoch [32/100] Completed\n",
      "[==============] Average d_loss: 0.3866 - Average g_loss: 3.6099\n",
      "Epoch [33/100], Step [0/938], d_loss: 0.3711, g_loss: 2.6752\n",
      "Epoch [33/100], Step [100/938], d_loss: 0.3581, g_loss: 3.8568\n",
      "Epoch [33/100], Step [200/938], d_loss: 0.4163, g_loss: 3.3790\n",
      "Epoch [33/100], Step [300/938], d_loss: 0.3026, g_loss: 4.6285\n",
      "Epoch [33/100], Step [400/938], d_loss: 0.6003, g_loss: 3.0589\n",
      "Epoch [33/100], Step [500/938], d_loss: 0.5058, g_loss: 3.4343\n",
      "Epoch [33/100], Step [600/938], d_loss: 0.3080, g_loss: 4.0315\n",
      "Epoch [33/100], Step [700/938], d_loss: 0.3102, g_loss: 4.3268\n",
      "Epoch [33/100], Step [800/938], d_loss: 0.3281, g_loss: 4.3498\n",
      "Epoch [33/100], Step [900/938], d_loss: 0.3661, g_loss: 4.1481\n",
      "Epoch [33/100] Completed\n",
      "[==============] Average d_loss: 0.3830 - Average g_loss: 3.5659\n",
      "Epoch [34/100], Step [0/938], d_loss: 0.4314, g_loss: 4.5860\n",
      "Epoch [34/100], Step [100/938], d_loss: 0.3939, g_loss: 3.0121\n",
      "Epoch [34/100], Step [200/938], d_loss: 0.3542, g_loss: 3.8519\n",
      "Epoch [34/100], Step [300/938], d_loss: 0.3683, g_loss: 4.3589\n",
      "Epoch [34/100], Step [400/938], d_loss: 0.4913, g_loss: 4.3247\n",
      "Epoch [34/100], Step [500/938], d_loss: 0.3222, g_loss: 3.2584\n",
      "Epoch [34/100], Step [600/938], d_loss: 0.5099, g_loss: 4.6759\n",
      "Epoch [34/100], Step [700/938], d_loss: 0.4477, g_loss: 4.1964\n",
      "Epoch [34/100], Step [800/938], d_loss: 0.4920, g_loss: 3.4252\n",
      "Epoch [34/100], Step [900/938], d_loss: 0.3459, g_loss: 4.6644\n",
      "Epoch [34/100] Completed\n",
      "[==============] Average d_loss: 0.3736 - Average g_loss: 3.7158\n",
      "Epoch [35/100], Step [0/938], d_loss: 0.4128, g_loss: 3.3855\n",
      "Epoch [35/100], Step [100/938], d_loss: 0.4857, g_loss: 4.0098\n",
      "Epoch [35/100], Step [200/938], d_loss: 0.4479, g_loss: 4.3888\n",
      "Epoch [35/100], Step [300/938], d_loss: 0.3014, g_loss: 3.0572\n",
      "Epoch [35/100], Step [400/938], d_loss: 0.2983, g_loss: 3.6698\n",
      "Epoch [35/100], Step [500/938], d_loss: 0.3422, g_loss: 3.0439\n",
      "Epoch [35/100], Step [600/938], d_loss: 0.4458, g_loss: 3.3405\n",
      "Epoch [35/100], Step [700/938], d_loss: 0.2146, g_loss: 3.8418\n",
      "Epoch [35/100], Step [800/938], d_loss: 0.3198, g_loss: 4.4286\n",
      "Epoch [35/100], Step [900/938], d_loss: 0.3834, g_loss: 3.8919\n",
      "Epoch [35/100] Completed\n",
      "[==============] Average d_loss: 0.3798 - Average g_loss: 3.6546\n",
      "Epoch [36/100], Step [0/938], d_loss: 0.4032, g_loss: 3.2944\n",
      "Epoch [36/100], Step [100/938], d_loss: 0.4545, g_loss: 4.0888\n",
      "Epoch [36/100], Step [200/938], d_loss: 0.3035, g_loss: 4.1253\n",
      "Epoch [36/100], Step [300/938], d_loss: 0.3133, g_loss: 3.9779\n",
      "Epoch [36/100], Step [400/938], d_loss: 0.2513, g_loss: 4.3533\n",
      "Epoch [36/100], Step [500/938], d_loss: 0.3680, g_loss: 3.2443\n",
      "Epoch [36/100], Step [600/938], d_loss: 0.3738, g_loss: 4.9509\n",
      "Epoch [36/100], Step [700/938], d_loss: 0.3175, g_loss: 4.8604\n",
      "Epoch [36/100], Step [800/938], d_loss: 0.3294, g_loss: 4.9027\n",
      "Epoch [36/100], Step [900/938], d_loss: 0.2156, g_loss: 3.9655\n",
      "Epoch [36/100] Completed\n",
      "[==============] Average d_loss: 0.3635 - Average g_loss: 3.8562\n",
      "Epoch [37/100], Step [0/938], d_loss: 0.4366, g_loss: 3.3007\n",
      "Epoch [37/100], Step [100/938], d_loss: 0.2642, g_loss: 3.5798\n",
      "Epoch [37/100], Step [200/938], d_loss: 0.3245, g_loss: 4.5963\n",
      "Epoch [37/100], Step [300/938], d_loss: 0.4765, g_loss: 4.3371\n",
      "Epoch [37/100], Step [400/938], d_loss: 0.4653, g_loss: 4.4472\n",
      "Epoch [37/100], Step [500/938], d_loss: 0.2656, g_loss: 3.5331\n",
      "Epoch [37/100], Step [600/938], d_loss: 0.3809, g_loss: 2.5999\n",
      "Epoch [37/100], Step [700/938], d_loss: 0.3682, g_loss: 3.2769\n",
      "Epoch [37/100], Step [800/938], d_loss: 0.2600, g_loss: 4.0578\n",
      "Epoch [37/100], Step [900/938], d_loss: 0.3287, g_loss: 3.0235\n",
      "Epoch [37/100] Completed\n",
      "[==============] Average d_loss: 0.3507 - Average g_loss: 3.7424\n",
      "Epoch [38/100], Step [0/938], d_loss: 0.4395, g_loss: 5.4698\n",
      "Epoch [38/100], Step [100/938], d_loss: 0.3603, g_loss: 4.1530\n",
      "Epoch [38/100], Step [200/938], d_loss: 0.6343, g_loss: 4.6018\n",
      "Epoch [38/100], Step [300/938], d_loss: 0.2337, g_loss: 3.2385\n",
      "Epoch [38/100], Step [400/938], d_loss: 0.3227, g_loss: 3.7824\n",
      "Epoch [38/100], Step [500/938], d_loss: 0.3318, g_loss: 3.4625\n",
      "Epoch [38/100], Step [600/938], d_loss: 0.3367, g_loss: 5.0098\n",
      "Epoch [38/100], Step [700/938], d_loss: 0.3098, g_loss: 4.4646\n",
      "Epoch [38/100], Step [800/938], d_loss: 0.2337, g_loss: 4.2131\n",
      "Epoch [38/100], Step [900/938], d_loss: 0.3850, g_loss: 5.1865\n",
      "Epoch [38/100] Completed\n",
      "[==============] Average d_loss: 0.3421 - Average g_loss: 3.8420\n",
      "Epoch [39/100], Step [0/938], d_loss: 0.2358, g_loss: 3.9683\n",
      "Epoch [39/100], Step [100/938], d_loss: 0.4165, g_loss: 3.7688\n",
      "Epoch [39/100], Step [200/938], d_loss: 0.3289, g_loss: 4.1165\n",
      "Epoch [39/100], Step [300/938], d_loss: 0.4123, g_loss: 4.2233\n",
      "Epoch [39/100], Step [400/938], d_loss: 0.1424, g_loss: 4.1478\n",
      "Epoch [39/100], Step [500/938], d_loss: 0.3359, g_loss: 3.3486\n",
      "Epoch [39/100], Step [600/938], d_loss: 0.2212, g_loss: 4.0922\n",
      "Epoch [39/100], Step [700/938], d_loss: 0.2897, g_loss: 3.2215\n",
      "Epoch [39/100], Step [800/938], d_loss: 0.3021, g_loss: 4.2839\n",
      "Epoch [39/100], Step [900/938], d_loss: 0.4577, g_loss: 3.3067\n",
      "Epoch [39/100] Completed\n",
      "[==============] Average d_loss: 0.3353 - Average g_loss: 3.9264\n",
      "Epoch [40/100], Step [0/938], d_loss: 0.4260, g_loss: 4.0085\n",
      "Epoch [40/100], Step [100/938], d_loss: 0.3218, g_loss: 3.6939\n",
      "Epoch [40/100], Step [200/938], d_loss: 0.4838, g_loss: 3.3467\n",
      "Epoch [40/100], Step [300/938], d_loss: 0.3732, g_loss: 2.9042\n",
      "Epoch [40/100], Step [400/938], d_loss: 0.3563, g_loss: 3.8742\n",
      "Epoch [40/100], Step [500/938], d_loss: 0.4176, g_loss: 2.7008\n",
      "Epoch [40/100], Step [600/938], d_loss: 0.3585, g_loss: 3.6238\n",
      "Epoch [40/100], Step [700/938], d_loss: 0.4452, g_loss: 5.8451\n",
      "Epoch [40/100], Step [800/938], d_loss: 0.4180, g_loss: 3.6943\n",
      "Epoch [40/100], Step [900/938], d_loss: 0.2645, g_loss: 3.9459\n",
      "Epoch [40/100] Completed\n",
      "[==============] Average d_loss: 0.3334 - Average g_loss: 3.8866\n",
      "Epoch [41/100], Step [0/938], d_loss: 0.4015, g_loss: 3.4542\n",
      "Epoch [41/100], Step [100/938], d_loss: 0.3804, g_loss: 4.3289\n",
      "Epoch [41/100], Step [200/938], d_loss: 0.4026, g_loss: 2.8600\n",
      "Epoch [41/100], Step [300/938], d_loss: 0.2663, g_loss: 3.3022\n",
      "Epoch [41/100], Step [400/938], d_loss: 0.2138, g_loss: 4.1871\n",
      "Epoch [41/100], Step [500/938], d_loss: 0.3022, g_loss: 4.3308\n",
      "Epoch [41/100], Step [600/938], d_loss: 0.2093, g_loss: 4.3069\n",
      "Epoch [41/100], Step [700/938], d_loss: 0.2881, g_loss: 4.1162\n",
      "Epoch [41/100], Step [800/938], d_loss: 0.3245, g_loss: 4.3515\n",
      "Epoch [41/100], Step [900/938], d_loss: 0.3806, g_loss: 3.0730\n",
      "Epoch [41/100] Completed\n",
      "[==============] Average d_loss: 0.3294 - Average g_loss: 3.9349\n",
      "Epoch [42/100], Step [0/938], d_loss: 0.2430, g_loss: 4.3104\n",
      "Epoch [42/100], Step [100/938], d_loss: 0.2435, g_loss: 4.0986\n",
      "Epoch [42/100], Step [200/938], d_loss: 0.2377, g_loss: 4.1084\n",
      "Epoch [42/100], Step [300/938], d_loss: 0.3642, g_loss: 2.8491\n",
      "Epoch [42/100], Step [400/938], d_loss: 0.2701, g_loss: 4.2524\n",
      "Epoch [42/100], Step [500/938], d_loss: 0.3673, g_loss: 4.5722\n",
      "Epoch [42/100], Step [600/938], d_loss: 0.2578, g_loss: 3.0601\n",
      "Epoch [42/100], Step [700/938], d_loss: 0.2432, g_loss: 3.6734\n",
      "Epoch [42/100], Step [800/938], d_loss: 0.2190, g_loss: 3.9628\n",
      "Epoch [42/100], Step [900/938], d_loss: 0.3083, g_loss: 4.0241\n",
      "Epoch [42/100] Completed\n",
      "[==============] Average d_loss: 0.3200 - Average g_loss: 3.9528\n",
      "Epoch [43/100], Step [0/938], d_loss: 0.2158, g_loss: 4.4362\n",
      "Epoch [43/100], Step [100/938], d_loss: 0.2861, g_loss: 4.0519\n",
      "Epoch [43/100], Step [200/938], d_loss: 0.2815, g_loss: 4.5873\n",
      "Epoch [43/100], Step [300/938], d_loss: 0.2795, g_loss: 4.6636\n",
      "Epoch [43/100], Step [400/938], d_loss: 0.2255, g_loss: 3.7982\n",
      "Epoch [43/100], Step [500/938], d_loss: 0.2635, g_loss: 4.2072\n",
      "Epoch [43/100], Step [600/938], d_loss: 0.3364, g_loss: 3.1858\n",
      "Epoch [43/100], Step [700/938], d_loss: 0.2901, g_loss: 3.5589\n",
      "Epoch [43/100], Step [800/938], d_loss: 0.3116, g_loss: 4.3290\n",
      "Epoch [43/100], Step [900/938], d_loss: 0.3144, g_loss: 4.0352\n",
      "Epoch [43/100] Completed\n",
      "[==============] Average d_loss: 0.3153 - Average g_loss: 4.0994\n",
      "Epoch [44/100], Step [0/938], d_loss: 0.3415, g_loss: 3.7707\n",
      "Epoch [44/100], Step [100/938], d_loss: 0.5097, g_loss: 3.1093\n",
      "Epoch [44/100], Step [200/938], d_loss: 0.3086, g_loss: 5.5984\n",
      "Epoch [44/100], Step [300/938], d_loss: 0.2286, g_loss: 3.8921\n",
      "Epoch [44/100], Step [400/938], d_loss: 0.4357, g_loss: 3.9274\n",
      "Epoch [44/100], Step [500/938], d_loss: 0.3516, g_loss: 4.5071\n",
      "Epoch [44/100], Step [600/938], d_loss: 0.3882, g_loss: 3.8528\n",
      "Epoch [44/100], Step [700/938], d_loss: 0.2180, g_loss: 4.0735\n",
      "Epoch [44/100], Step [800/938], d_loss: 0.3068, g_loss: 3.4308\n",
      "Epoch [44/100], Step [900/938], d_loss: 0.3190, g_loss: 3.0148\n",
      "Epoch [44/100] Completed\n",
      "[==============] Average d_loss: 0.3249 - Average g_loss: 3.9576\n",
      "Epoch [45/100], Step [0/938], d_loss: 0.3180, g_loss: 3.2130\n",
      "Epoch [45/100], Step [100/938], d_loss: 0.3045, g_loss: 4.8735\n",
      "Epoch [45/100], Step [200/938], d_loss: 0.2545, g_loss: 4.3010\n",
      "Epoch [45/100], Step [300/938], d_loss: 0.3228, g_loss: 3.9304\n",
      "Epoch [45/100], Step [400/938], d_loss: 0.3595, g_loss: 2.8741\n",
      "Epoch [45/100], Step [500/938], d_loss: 0.3308, g_loss: 5.6430\n",
      "Epoch [45/100], Step [600/938], d_loss: 0.3726, g_loss: 3.4903\n",
      "Epoch [45/100], Step [700/938], d_loss: 0.2940, g_loss: 4.2292\n",
      "Epoch [45/100], Step [800/938], d_loss: 0.2607, g_loss: 4.7017\n",
      "Epoch [45/100], Step [900/938], d_loss: 0.1881, g_loss: 5.5789\n",
      "Epoch [45/100] Completed\n",
      "[==============] Average d_loss: 0.3163 - Average g_loss: 3.9858\n",
      "Epoch [46/100], Step [0/938], d_loss: 0.3594, g_loss: 4.9398\n",
      "Epoch [46/100], Step [100/938], d_loss: 0.3650, g_loss: 4.0513\n",
      "Epoch [46/100], Step [200/938], d_loss: 0.3848, g_loss: 5.3171\n",
      "Epoch [46/100], Step [300/938], d_loss: 0.2722, g_loss: 3.3341\n",
      "Epoch [46/100], Step [400/938], d_loss: 0.3372, g_loss: 4.3724\n",
      "Epoch [46/100], Step [500/938], d_loss: 0.3722, g_loss: 3.3907\n",
      "Epoch [46/100], Step [600/938], d_loss: 0.2356, g_loss: 4.0752\n",
      "Epoch [46/100], Step [700/938], d_loss: 0.3610, g_loss: 3.8708\n",
      "Epoch [46/100], Step [800/938], d_loss: 0.4329, g_loss: 4.4286\n",
      "Epoch [46/100], Step [900/938], d_loss: 0.3335, g_loss: 4.2126\n",
      "Epoch [46/100] Completed\n",
      "[==============] Average d_loss: 0.3170 - Average g_loss: 4.1203\n",
      "Epoch [47/100], Step [0/938], d_loss: 0.2718, g_loss: 4.9306\n",
      "Epoch [47/100], Step [100/938], d_loss: 0.2018, g_loss: 4.3474\n",
      "Epoch [47/100], Step [200/938], d_loss: 0.2781, g_loss: 3.9881\n",
      "Epoch [47/100], Step [300/938], d_loss: 0.3191, g_loss: 3.6928\n",
      "Epoch [47/100], Step [400/938], d_loss: 0.2673, g_loss: 3.9000\n",
      "Epoch [47/100], Step [500/938], d_loss: 0.3797, g_loss: 3.5661\n",
      "Epoch [47/100], Step [600/938], d_loss: 0.4978, g_loss: 5.7847\n",
      "Epoch [47/100], Step [700/938], d_loss: 0.2040, g_loss: 4.1676\n",
      "Epoch [47/100], Step [800/938], d_loss: 0.2438, g_loss: 3.8056\n",
      "Epoch [47/100], Step [900/938], d_loss: 0.3687, g_loss: 3.1217\n",
      "Epoch [47/100] Completed\n",
      "[==============] Average d_loss: 0.2984 - Average g_loss: 4.0536\n",
      "Epoch [48/100], Step [0/938], d_loss: 0.2762, g_loss: 3.6640\n",
      "Epoch [48/100], Step [100/938], d_loss: 0.3239, g_loss: 3.5437\n",
      "Epoch [48/100], Step [200/938], d_loss: 0.3959, g_loss: 4.8010\n",
      "Epoch [48/100], Step [300/938], d_loss: 0.4319, g_loss: 4.5011\n",
      "Epoch [48/100], Step [400/938], d_loss: 0.1614, g_loss: 4.7927\n",
      "Epoch [48/100], Step [500/938], d_loss: 0.4137, g_loss: 4.3689\n",
      "Epoch [48/100], Step [600/938], d_loss: 0.3529, g_loss: 4.3411\n",
      "Epoch [48/100], Step [700/938], d_loss: 0.2047, g_loss: 4.2640\n",
      "Epoch [48/100], Step [800/938], d_loss: 0.1387, g_loss: 3.4768\n",
      "Epoch [48/100], Step [900/938], d_loss: 0.2776, g_loss: 5.3127\n",
      "Epoch [48/100] Completed\n",
      "[==============] Average d_loss: 0.3016 - Average g_loss: 4.3929\n",
      "Epoch [49/100], Step [0/938], d_loss: 0.1538, g_loss: 6.1673\n",
      "Epoch [49/100], Step [100/938], d_loss: 0.2749, g_loss: 4.1018\n",
      "Epoch [49/100], Step [200/938], d_loss: 0.2910, g_loss: 3.5928\n",
      "Epoch [49/100], Step [300/938], d_loss: 0.2125, g_loss: 4.7884\n",
      "Epoch [49/100], Step [400/938], d_loss: 0.1981, g_loss: 4.5599\n",
      "Epoch [49/100], Step [500/938], d_loss: 0.3112, g_loss: 5.1833\n",
      "Epoch [49/100], Step [600/938], d_loss: 0.2849, g_loss: 4.5927\n",
      "Epoch [49/100], Step [700/938], d_loss: 0.3410, g_loss: 4.2703\n",
      "Epoch [49/100], Step [800/938], d_loss: 0.2673, g_loss: 3.7837\n",
      "Epoch [49/100], Step [900/938], d_loss: 0.4086, g_loss: 4.7915\n",
      "Epoch [49/100] Completed\n",
      "[==============] Average d_loss: 0.2979 - Average g_loss: 4.2260\n",
      "Epoch [50/100], Step [0/938], d_loss: 0.3404, g_loss: 5.7478\n",
      "Epoch [50/100], Step [100/938], d_loss: 0.2624, g_loss: 4.8875\n",
      "Epoch [50/100], Step [200/938], d_loss: 0.3362, g_loss: 4.9923\n",
      "Epoch [50/100], Step [300/938], d_loss: 0.5585, g_loss: 2.2801\n",
      "Epoch [50/100], Step [400/938], d_loss: 0.4426, g_loss: 6.2331\n",
      "Epoch [50/100], Step [500/938], d_loss: 0.2415, g_loss: 4.3183\n",
      "Epoch [50/100], Step [600/938], d_loss: 0.2019, g_loss: 4.3565\n",
      "Epoch [50/100], Step [700/938], d_loss: 0.2780, g_loss: 4.8048\n",
      "Epoch [50/100], Step [800/938], d_loss: 0.1813, g_loss: 3.7635\n",
      "Epoch [50/100], Step [900/938], d_loss: 0.2694, g_loss: 3.9607\n",
      "Epoch [50/100] Completed\n",
      "[==============] Average d_loss: 0.2685 - Average g_loss: 4.2911\n",
      "Epoch [51/100], Step [0/938], d_loss: 0.1962, g_loss: 5.5014\n",
      "Epoch [51/100], Step [100/938], d_loss: 0.1979, g_loss: 4.4165\n",
      "Epoch [51/100], Step [200/938], d_loss: 0.2217, g_loss: 4.0674\n",
      "Epoch [51/100], Step [300/938], d_loss: 0.2045, g_loss: 3.5768\n",
      "Epoch [51/100], Step [400/938], d_loss: 0.2603, g_loss: 4.3022\n",
      "Epoch [51/100], Step [500/938], d_loss: 0.4142, g_loss: 2.6954\n",
      "Epoch [51/100], Step [600/938], d_loss: 0.3364, g_loss: 5.8466\n",
      "Epoch [51/100], Step [700/938], d_loss: 0.2345, g_loss: 4.1819\n",
      "Epoch [51/100], Step [800/938], d_loss: 0.2531, g_loss: 4.0935\n",
      "Epoch [51/100], Step [900/938], d_loss: 0.4939, g_loss: 3.7936\n",
      "Epoch [51/100] Completed\n",
      "[==============] Average d_loss: 0.2861 - Average g_loss: 4.3828\n",
      "Epoch [52/100], Step [0/938], d_loss: 0.4222, g_loss: 6.0815\n",
      "Epoch [52/100], Step [100/938], d_loss: 0.2525, g_loss: 3.3806\n",
      "Epoch [52/100], Step [200/938], d_loss: 0.3003, g_loss: 4.4156\n",
      "Epoch [52/100], Step [300/938], d_loss: 0.3561, g_loss: 4.1813\n",
      "Epoch [52/100], Step [400/938], d_loss: 0.3702, g_loss: 3.7679\n",
      "Epoch [52/100], Step [500/938], d_loss: 0.1972, g_loss: 4.8930\n",
      "Epoch [52/100], Step [600/938], d_loss: 0.3381, g_loss: 4.0812\n",
      "Epoch [52/100], Step [700/938], d_loss: 0.2990, g_loss: 3.5098\n",
      "Epoch [52/100], Step [800/938], d_loss: 0.3079, g_loss: 3.6326\n",
      "Epoch [52/100], Step [900/938], d_loss: 0.3521, g_loss: 3.6941\n",
      "Epoch [52/100] Completed\n",
      "[==============] Average d_loss: 0.2847 - Average g_loss: 4.2839\n",
      "Epoch [53/100], Step [0/938], d_loss: 0.3063, g_loss: 4.7418\n",
      "Epoch [53/100], Step [100/938], d_loss: 0.2153, g_loss: 4.3994\n",
      "Epoch [53/100], Step [200/938], d_loss: 0.1702, g_loss: 4.1924\n",
      "Epoch [53/100], Step [300/938], d_loss: 0.3320, g_loss: 3.2394\n",
      "Epoch [53/100], Step [400/938], d_loss: 0.2267, g_loss: 4.3672\n",
      "Epoch [53/100], Step [500/938], d_loss: 0.1997, g_loss: 3.5985\n",
      "Epoch [53/100], Step [600/938], d_loss: 0.3701, g_loss: 4.2861\n",
      "Epoch [53/100], Step [700/938], d_loss: 0.3442, g_loss: 5.6771\n",
      "Epoch [53/100], Step [800/938], d_loss: 0.2222, g_loss: 3.9334\n",
      "Epoch [53/100], Step [900/938], d_loss: 0.3546, g_loss: 3.8565\n",
      "Epoch [53/100] Completed\n",
      "[==============] Average d_loss: 0.2872 - Average g_loss: 4.2723\n",
      "Epoch [54/100], Step [0/938], d_loss: 0.2317, g_loss: 4.3925\n",
      "Epoch [54/100], Step [100/938], d_loss: 0.2950, g_loss: 3.8481\n",
      "Epoch [54/100], Step [200/938], d_loss: 0.3241, g_loss: 5.7186\n",
      "Epoch [54/100], Step [300/938], d_loss: 0.2587, g_loss: 4.5013\n",
      "Epoch [54/100], Step [400/938], d_loss: 0.2687, g_loss: 5.6098\n",
      "Epoch [54/100], Step [500/938], d_loss: 0.3868, g_loss: 4.2909\n",
      "Epoch [54/100], Step [600/938], d_loss: 0.2854, g_loss: 3.8197\n",
      "Epoch [54/100], Step [700/938], d_loss: 0.1229, g_loss: 5.6745\n",
      "Epoch [54/100], Step [800/938], d_loss: 0.2481, g_loss: 3.8984\n",
      "Epoch [54/100], Step [900/938], d_loss: 0.3713, g_loss: 4.6787\n",
      "Epoch [54/100] Completed\n",
      "[==============] Average d_loss: 0.2738 - Average g_loss: 4.3568\n",
      "Epoch [55/100], Step [0/938], d_loss: 0.2046, g_loss: 3.6916\n",
      "Epoch [55/100], Step [100/938], d_loss: 0.2506, g_loss: 4.3141\n",
      "Epoch [55/100], Step [200/938], d_loss: 0.3073, g_loss: 4.0353\n",
      "Epoch [55/100], Step [300/938], d_loss: 0.1950, g_loss: 5.6583\n",
      "Epoch [55/100], Step [400/938], d_loss: 0.3455, g_loss: 3.3410\n",
      "Epoch [55/100], Step [500/938], d_loss: 0.3163, g_loss: 5.5405\n",
      "Epoch [55/100], Step [600/938], d_loss: 0.2800, g_loss: 5.1012\n",
      "Epoch [55/100], Step [700/938], d_loss: 0.3421, g_loss: 5.5541\n",
      "Epoch [55/100], Step [800/938], d_loss: 0.4096, g_loss: 5.4930\n",
      "Epoch [55/100], Step [900/938], d_loss: 0.2824, g_loss: 3.0881\n",
      "Epoch [55/100] Completed\n",
      "[==============] Average d_loss: 0.2633 - Average g_loss: 4.5358\n",
      "Epoch [56/100], Step [0/938], d_loss: 0.4814, g_loss: 6.2939\n",
      "Epoch [56/100], Step [100/938], d_loss: 0.2750, g_loss: 4.6229\n",
      "Epoch [56/100], Step [200/938], d_loss: 0.2819, g_loss: 4.5211\n",
      "Epoch [56/100], Step [300/938], d_loss: 0.2214, g_loss: 4.5445\n",
      "Epoch [56/100], Step [400/938], d_loss: 0.2533, g_loss: 4.6750\n",
      "Epoch [56/100], Step [500/938], d_loss: 0.4250, g_loss: 3.7660\n",
      "Epoch [56/100], Step [600/938], d_loss: 0.2384, g_loss: 5.2702\n",
      "Epoch [56/100], Step [700/938], d_loss: 0.1433, g_loss: 4.2028\n",
      "Epoch [56/100], Step [800/938], d_loss: 0.3473, g_loss: 3.5713\n",
      "Epoch [56/100], Step [900/938], d_loss: 0.3833, g_loss: 4.7971\n",
      "Epoch [56/100] Completed\n",
      "[==============] Average d_loss: 0.2843 - Average g_loss: 4.4829\n",
      "Epoch [57/100], Step [0/938], d_loss: 0.1903, g_loss: 4.6568\n",
      "Epoch [57/100], Step [100/938], d_loss: 0.2227, g_loss: 3.7730\n",
      "Epoch [57/100], Step [200/938], d_loss: 0.2105, g_loss: 4.0190\n",
      "Epoch [57/100], Step [300/938], d_loss: 0.2115, g_loss: 4.0415\n",
      "Epoch [57/100], Step [400/938], d_loss: 0.1893, g_loss: 4.5565\n",
      "Epoch [57/100], Step [500/938], d_loss: 0.4805, g_loss: 7.2768\n",
      "Epoch [57/100], Step [600/938], d_loss: 0.2120, g_loss: 4.8954\n",
      "Epoch [57/100], Step [700/938], d_loss: 0.3753, g_loss: 3.5036\n",
      "Epoch [57/100], Step [800/938], d_loss: 0.1783, g_loss: 4.1992\n",
      "Epoch [57/100], Step [900/938], d_loss: 0.1881, g_loss: 4.4978\n",
      "Epoch [57/100] Completed\n",
      "[==============] Average d_loss: 0.2869 - Average g_loss: 4.4265\n",
      "Epoch [58/100], Step [0/938], d_loss: 0.2838, g_loss: 3.6230\n",
      "Epoch [58/100], Step [100/938], d_loss: 0.2437, g_loss: 4.0911\n",
      "Epoch [58/100], Step [200/938], d_loss: 0.1389, g_loss: 5.6080\n",
      "Epoch [58/100], Step [300/938], d_loss: 0.2635, g_loss: 4.6444\n",
      "Epoch [58/100], Step [400/938], d_loss: 0.2806, g_loss: 3.5654\n",
      "Epoch [58/100], Step [500/938], d_loss: 0.1877, g_loss: 4.6862\n",
      "Epoch [58/100], Step [600/938], d_loss: 0.1872, g_loss: 5.1647\n",
      "Epoch [58/100], Step [700/938], d_loss: 0.2431, g_loss: 4.2157\n",
      "Epoch [58/100], Step [800/938], d_loss: 0.2007, g_loss: 4.2073\n",
      "Epoch [58/100], Step [900/938], d_loss: 0.1967, g_loss: 5.4466\n",
      "Epoch [58/100] Completed\n",
      "[==============] Average d_loss: 0.2747 - Average g_loss: 4.4489\n",
      "Epoch [59/100], Step [0/938], d_loss: 0.2493, g_loss: 4.6291\n",
      "Epoch [59/100], Step [100/938], d_loss: 0.2267, g_loss: 5.1746\n",
      "Epoch [59/100], Step [200/938], d_loss: 0.1875, g_loss: 4.9807\n",
      "Epoch [59/100], Step [300/938], d_loss: 0.3462, g_loss: 4.9110\n",
      "Epoch [59/100], Step [400/938], d_loss: 0.2519, g_loss: 4.1841\n",
      "Epoch [59/100], Step [500/938], d_loss: 0.3370, g_loss: 3.4092\n",
      "Epoch [59/100], Step [600/938], d_loss: 0.3264, g_loss: 3.6324\n",
      "Epoch [59/100], Step [700/938], d_loss: 0.1573, g_loss: 3.9970\n",
      "Epoch [59/100], Step [800/938], d_loss: 0.1676, g_loss: 4.6536\n",
      "Epoch [59/100], Step [900/938], d_loss: 0.5357, g_loss: 6.1676\n",
      "Epoch [59/100] Completed\n",
      "[==============] Average d_loss: 0.2550 - Average g_loss: 4.4284\n",
      "Epoch [60/100], Step [0/938], d_loss: 0.1429, g_loss: 4.5437\n",
      "Epoch [60/100], Step [100/938], d_loss: 0.1900, g_loss: 4.5045\n",
      "Epoch [60/100], Step [200/938], d_loss: 0.2459, g_loss: 4.4412\n",
      "Epoch [60/100], Step [300/938], d_loss: 0.2819, g_loss: 4.1374\n",
      "Epoch [60/100], Step [400/938], d_loss: 0.3219, g_loss: 3.9470\n",
      "Epoch [60/100], Step [500/938], d_loss: 0.4198, g_loss: 6.5934\n",
      "Epoch [60/100], Step [600/938], d_loss: 0.2734, g_loss: 4.8394\n",
      "Epoch [60/100], Step [700/938], d_loss: 0.4120, g_loss: 2.5344\n",
      "Epoch [60/100], Step [800/938], d_loss: 0.2375, g_loss: 4.6893\n",
      "Epoch [60/100], Step [900/938], d_loss: 0.2779, g_loss: 4.0244\n",
      "Epoch [60/100] Completed\n",
      "[==============] Average d_loss: 0.2723 - Average g_loss: 4.5696\n",
      "Epoch [61/100], Step [0/938], d_loss: 0.1618, g_loss: 5.3218\n",
      "Epoch [61/100], Step [100/938], d_loss: 0.4475, g_loss: 6.7471\n",
      "Epoch [61/100], Step [200/938], d_loss: 0.1828, g_loss: 4.6372\n",
      "Epoch [61/100], Step [300/938], d_loss: 0.3006, g_loss: 5.1584\n",
      "Epoch [61/100], Step [400/938], d_loss: 0.3283, g_loss: 4.4070\n",
      "Epoch [61/100], Step [500/938], d_loss: 0.1849, g_loss: 4.5342\n",
      "Epoch [61/100], Step [600/938], d_loss: 0.2852, g_loss: 3.4587\n",
      "Epoch [61/100], Step [700/938], d_loss: 0.3957, g_loss: 3.7127\n",
      "Epoch [61/100], Step [800/938], d_loss: 0.1693, g_loss: 4.1511\n",
      "Epoch [61/100], Step [900/938], d_loss: 0.2818, g_loss: 4.1508\n",
      "Epoch [61/100] Completed\n",
      "[==============] Average d_loss: 0.2816 - Average g_loss: 4.5357\n",
      "Epoch [62/100], Step [0/938], d_loss: 0.2037, g_loss: 4.6288\n",
      "Epoch [62/100], Step [100/938], d_loss: 0.2927, g_loss: 4.7603\n",
      "Epoch [62/100], Step [200/938], d_loss: 0.4273, g_loss: 6.5317\n",
      "Epoch [62/100], Step [300/938], d_loss: 0.3107, g_loss: 5.4977\n",
      "Epoch [62/100], Step [400/938], d_loss: 0.2103, g_loss: 3.8638\n",
      "Epoch [62/100], Step [500/938], d_loss: 0.1902, g_loss: 4.3483\n",
      "Epoch [62/100], Step [600/938], d_loss: 0.3009, g_loss: 3.7733\n",
      "Epoch [62/100], Step [700/938], d_loss: 0.1598, g_loss: 4.6431\n",
      "Epoch [62/100], Step [800/938], d_loss: 0.1912, g_loss: 4.9807\n",
      "Epoch [62/100], Step [900/938], d_loss: 0.2360, g_loss: 3.2543\n",
      "Epoch [62/100] Completed\n",
      "[==============] Average d_loss: 0.2645 - Average g_loss: 4.4567\n",
      "Epoch [63/100], Step [0/938], d_loss: 0.2627, g_loss: 3.9584\n",
      "Epoch [63/100], Step [100/938], d_loss: 0.2622, g_loss: 4.3473\n",
      "Epoch [63/100], Step [200/938], d_loss: 0.1729, g_loss: 3.5410\n",
      "Epoch [63/100], Step [300/938], d_loss: 0.2950, g_loss: 5.6378\n",
      "Epoch [63/100], Step [400/938], d_loss: 0.2161, g_loss: 4.6825\n",
      "Epoch [63/100], Step [500/938], d_loss: 0.2880, g_loss: 3.4255\n",
      "Epoch [63/100], Step [600/938], d_loss: 0.2860, g_loss: 4.7866\n",
      "Epoch [63/100], Step [700/938], d_loss: 0.3166, g_loss: 5.9471\n",
      "Epoch [63/100], Step [800/938], d_loss: 0.2405, g_loss: 4.3359\n",
      "Epoch [63/100], Step [900/938], d_loss: 0.2697, g_loss: 4.2361\n",
      "Epoch [63/100] Completed\n",
      "[==============] Average d_loss: 0.2816 - Average g_loss: 4.4463\n",
      "Epoch [64/100], Step [0/938], d_loss: 0.4559, g_loss: 3.4120\n",
      "Epoch [64/100], Step [100/938], d_loss: 0.1982, g_loss: 4.3814\n",
      "Epoch [64/100], Step [200/938], d_loss: 0.1554, g_loss: 4.4732\n",
      "Epoch [64/100], Step [300/938], d_loss: 0.2940, g_loss: 3.7106\n",
      "Epoch [64/100], Step [400/938], d_loss: 0.1670, g_loss: 4.7270\n",
      "Epoch [64/100], Step [500/938], d_loss: 0.2351, g_loss: 3.4804\n",
      "Epoch [64/100], Step [600/938], d_loss: 0.3269, g_loss: 4.3015\n",
      "Epoch [64/100], Step [700/938], d_loss: 0.2151, g_loss: 4.1890\n",
      "Epoch [64/100], Step [800/938], d_loss: 0.2433, g_loss: 4.9359\n",
      "Epoch [64/100], Step [900/938], d_loss: 0.3501, g_loss: 3.8277\n",
      "Epoch [64/100] Completed\n",
      "[==============] Average d_loss: 0.2673 - Average g_loss: 4.3372\n",
      "Epoch [65/100], Step [0/938], d_loss: 0.3296, g_loss: 4.1155\n",
      "Epoch [65/100], Step [100/938], d_loss: 0.2167, g_loss: 4.3683\n",
      "Epoch [65/100], Step [200/938], d_loss: 0.2288, g_loss: 4.1853\n",
      "Epoch [65/100], Step [300/938], d_loss: 0.2278, g_loss: 3.3101\n",
      "Epoch [65/100], Step [400/938], d_loss: 0.4740, g_loss: 3.0973\n",
      "Epoch [65/100], Step [500/938], d_loss: 0.3236, g_loss: 3.4204\n",
      "Epoch [65/100], Step [600/938], d_loss: 0.1968, g_loss: 4.8092\n",
      "Epoch [65/100], Step [700/938], d_loss: 0.1809, g_loss: 4.1684\n",
      "Epoch [65/100], Step [800/938], d_loss: 0.3613, g_loss: 6.2678\n",
      "Epoch [65/100], Step [900/938], d_loss: 0.3037, g_loss: 5.4799\n",
      "Epoch [65/100] Completed\n",
      "[==============] Average d_loss: 0.2599 - Average g_loss: 4.5718\n",
      "Epoch [66/100], Step [0/938], d_loss: 0.2308, g_loss: 4.9265\n",
      "Epoch [66/100], Step [100/938], d_loss: 0.2851, g_loss: 4.5348\n",
      "Epoch [66/100], Step [200/938], d_loss: 0.1559, g_loss: 4.6157\n",
      "Epoch [66/100], Step [300/938], d_loss: 0.2963, g_loss: 5.3934\n",
      "Epoch [66/100], Step [400/938], d_loss: 0.3186, g_loss: 4.3188\n",
      "Epoch [66/100], Step [500/938], d_loss: 0.3257, g_loss: 4.4499\n",
      "Epoch [66/100], Step [600/938], d_loss: 0.2081, g_loss: 4.8719\n",
      "Epoch [66/100], Step [700/938], d_loss: 0.3359, g_loss: 5.0516\n",
      "Epoch [66/100], Step [800/938], d_loss: 0.3152, g_loss: 3.9973\n",
      "Epoch [66/100], Step [900/938], d_loss: 0.4240, g_loss: 3.5115\n",
      "Epoch [66/100] Completed\n",
      "[==============] Average d_loss: 0.2824 - Average g_loss: 4.5297\n",
      "Epoch [67/100], Step [0/938], d_loss: 0.3156, g_loss: 4.2468\n",
      "Epoch [67/100], Step [100/938], d_loss: 0.2924, g_loss: 3.9173\n",
      "Epoch [67/100], Step [200/938], d_loss: 0.1801, g_loss: 4.7592\n",
      "Epoch [67/100], Step [300/938], d_loss: 0.2771, g_loss: 5.7167\n",
      "Epoch [67/100], Step [400/938], d_loss: 0.1514, g_loss: 4.3367\n",
      "Epoch [67/100], Step [500/938], d_loss: 0.1265, g_loss: 5.0206\n",
      "Epoch [67/100], Step [600/938], d_loss: 0.3485, g_loss: 4.1556\n",
      "Epoch [67/100], Step [700/938], d_loss: 0.4463, g_loss: 3.9149\n",
      "Epoch [67/100], Step [800/938], d_loss: 0.3740, g_loss: 3.2502\n",
      "Epoch [67/100], Step [900/938], d_loss: 0.3072, g_loss: 5.6379\n",
      "Epoch [67/100] Completed\n",
      "[==============] Average d_loss: 0.2653 - Average g_loss: 4.5339\n",
      "Epoch [68/100], Step [0/938], d_loss: 0.1202, g_loss: 5.2579\n",
      "Epoch [68/100], Step [100/938], d_loss: 0.3903, g_loss: 6.1182\n",
      "Epoch [68/100], Step [200/938], d_loss: 0.3035, g_loss: 4.1943\n",
      "Epoch [68/100], Step [300/938], d_loss: 0.3107, g_loss: 4.0875\n",
      "Epoch [68/100], Step [400/938], d_loss: 0.2745, g_loss: 3.2143\n",
      "Epoch [68/100], Step [500/938], d_loss: 0.3421, g_loss: 4.6785\n",
      "Epoch [68/100], Step [600/938], d_loss: 0.2808, g_loss: 4.3508\n",
      "Epoch [68/100], Step [700/938], d_loss: 0.2801, g_loss: 5.4235\n",
      "Epoch [68/100], Step [800/938], d_loss: 0.1141, g_loss: 4.2805\n",
      "Epoch [68/100], Step [900/938], d_loss: 0.3658, g_loss: 4.5471\n",
      "Epoch [68/100] Completed\n",
      "[==============] Average d_loss: 0.2696 - Average g_loss: 4.5385\n",
      "Epoch [69/100], Step [0/938], d_loss: 0.3186, g_loss: 3.3505\n",
      "Epoch [69/100], Step [100/938], d_loss: 0.3103, g_loss: 4.2631\n",
      "Epoch [69/100], Step [200/938], d_loss: 0.2587, g_loss: 3.8058\n",
      "Epoch [69/100], Step [300/938], d_loss: 0.2920, g_loss: 3.5882\n",
      "Epoch [69/100], Step [400/938], d_loss: 0.2198, g_loss: 4.6720\n",
      "Epoch [69/100], Step [500/938], d_loss: 0.2066, g_loss: 5.9095\n",
      "Epoch [69/100], Step [600/938], d_loss: 0.3081, g_loss: 3.9091\n",
      "Epoch [69/100], Step [700/938], d_loss: 0.1880, g_loss: 3.6088\n",
      "Epoch [69/100], Step [800/938], d_loss: 0.1640, g_loss: 4.4624\n",
      "Epoch [69/100], Step [900/938], d_loss: 0.3419, g_loss: 3.0706\n",
      "Epoch [69/100] Completed\n",
      "[==============] Average d_loss: 0.2537 - Average g_loss: 4.5781\n",
      "Epoch [70/100], Step [0/938], d_loss: 0.2222, g_loss: 6.0446\n",
      "Epoch [70/100], Step [100/938], d_loss: 0.1823, g_loss: 6.1090\n",
      "Epoch [70/100], Step [200/938], d_loss: 0.4086, g_loss: 6.1701\n",
      "Epoch [70/100], Step [300/938], d_loss: 0.3269, g_loss: 5.9782\n",
      "Epoch [70/100], Step [400/938], d_loss: 0.2107, g_loss: 5.3252\n",
      "Epoch [70/100], Step [500/938], d_loss: 0.2759, g_loss: 3.8164\n",
      "Epoch [70/100], Step [600/938], d_loss: 0.3689, g_loss: 5.7535\n",
      "Epoch [70/100], Step [700/938], d_loss: 0.1684, g_loss: 5.7240\n",
      "Epoch [70/100], Step [800/938], d_loss: 0.3640, g_loss: 3.9062\n",
      "Epoch [70/100], Step [900/938], d_loss: 0.3367, g_loss: 3.9186\n",
      "Epoch [70/100] Completed\n",
      "[==============] Average d_loss: 0.2791 - Average g_loss: 4.6197\n",
      "Epoch [71/100], Step [0/938], d_loss: 0.2882, g_loss: 3.6969\n",
      "Epoch [71/100], Step [100/938], d_loss: 0.3651, g_loss: 3.0970\n",
      "Epoch [71/100], Step [200/938], d_loss: 0.2557, g_loss: 4.4322\n",
      "Epoch [71/100], Step [300/938], d_loss: 0.3910, g_loss: 4.0071\n",
      "Epoch [71/100], Step [400/938], d_loss: 0.5088, g_loss: 6.0661\n",
      "Epoch [71/100], Step [500/938], d_loss: 0.3504, g_loss: 2.9168\n",
      "Epoch [71/100], Step [600/938], d_loss: 0.3406, g_loss: 3.1395\n",
      "Epoch [71/100], Step [700/938], d_loss: 0.3499, g_loss: 4.6659\n",
      "Epoch [71/100], Step [800/938], d_loss: 0.2645, g_loss: 5.7096\n",
      "Epoch [71/100], Step [900/938], d_loss: 0.2424, g_loss: 4.9105\n",
      "Epoch [71/100] Completed\n",
      "[==============] Average d_loss: 0.2803 - Average g_loss: 4.4144\n",
      "Epoch [72/100], Step [0/938], d_loss: 0.1799, g_loss: 4.3453\n",
      "Epoch [72/100], Step [100/938], d_loss: 0.2834, g_loss: 4.7961\n",
      "Epoch [72/100], Step [200/938], d_loss: 0.3139, g_loss: 4.0260\n",
      "Epoch [72/100], Step [300/938], d_loss: 0.3605, g_loss: 2.6530\n",
      "Epoch [72/100], Step [400/938], d_loss: 0.2670, g_loss: 5.1434\n",
      "Epoch [72/100], Step [500/938], d_loss: 0.3065, g_loss: 4.0205\n",
      "Epoch [72/100], Step [600/938], d_loss: 0.2313, g_loss: 3.6842\n",
      "Epoch [72/100], Step [700/938], d_loss: 0.2189, g_loss: 4.3335\n",
      "Epoch [72/100], Step [800/938], d_loss: 0.2173, g_loss: 3.7303\n",
      "Epoch [72/100], Step [900/938], d_loss: 0.1457, g_loss: 3.7547\n",
      "Epoch [72/100] Completed\n",
      "[==============] Average d_loss: 0.2670 - Average g_loss: 4.4077\n",
      "Epoch [73/100], Step [0/938], d_loss: 0.3006, g_loss: 3.3699\n",
      "Epoch [73/100], Step [100/938], d_loss: 0.2566, g_loss: 3.8551\n",
      "Epoch [73/100], Step [200/938], d_loss: 0.2108, g_loss: 4.8000\n",
      "Epoch [73/100], Step [300/938], d_loss: 0.1556, g_loss: 3.8836\n",
      "Epoch [73/100], Step [400/938], d_loss: 0.2943, g_loss: 4.8342\n",
      "Epoch [73/100], Step [500/938], d_loss: 0.3218, g_loss: 5.0191\n",
      "Epoch [73/100], Step [600/938], d_loss: 0.1600, g_loss: 5.5845\n",
      "Epoch [73/100], Step [700/938], d_loss: 0.4082, g_loss: 3.4428\n",
      "Epoch [73/100], Step [800/938], d_loss: 0.1823, g_loss: 4.3845\n",
      "Epoch [73/100], Step [900/938], d_loss: 0.3455, g_loss: 3.9321\n",
      "Epoch [73/100] Completed\n",
      "[==============] Average d_loss: 0.2721 - Average g_loss: 4.4771\n",
      "Epoch [74/100], Step [0/938], d_loss: 0.3103, g_loss: 5.0598\n",
      "Epoch [74/100], Step [100/938], d_loss: 0.3597, g_loss: 3.9966\n",
      "Epoch [74/100], Step [200/938], d_loss: 0.3832, g_loss: 4.6086\n",
      "Epoch [74/100], Step [300/938], d_loss: 0.2815, g_loss: 4.3115\n",
      "Epoch [74/100], Step [400/938], d_loss: 0.1601, g_loss: 4.0616\n",
      "Epoch [74/100], Step [500/938], d_loss: 0.2140, g_loss: 5.4601\n",
      "Epoch [74/100], Step [600/938], d_loss: 0.1708, g_loss: 3.8916\n",
      "Epoch [74/100], Step [700/938], d_loss: 0.1927, g_loss: 4.9901\n",
      "Epoch [74/100], Step [800/938], d_loss: 0.2132, g_loss: 3.8016\n",
      "Epoch [74/100], Step [900/938], d_loss: 0.2042, g_loss: 5.0619\n",
      "Epoch [74/100] Completed\n",
      "[==============] Average d_loss: 0.2658 - Average g_loss: 4.4886\n",
      "Epoch [75/100], Step [0/938], d_loss: 0.3138, g_loss: 5.7869\n",
      "Epoch [75/100], Step [100/938], d_loss: 0.2788, g_loss: 3.3339\n",
      "Epoch [75/100], Step [200/938], d_loss: 0.2547, g_loss: 5.1393\n",
      "Epoch [75/100], Step [300/938], d_loss: 0.3925, g_loss: 2.7515\n",
      "Epoch [75/100], Step [400/938], d_loss: 0.3325, g_loss: 3.7681\n",
      "Epoch [75/100], Step [500/938], d_loss: 0.1832, g_loss: 4.3992\n",
      "Epoch [75/100], Step [600/938], d_loss: 0.3145, g_loss: 4.2999\n",
      "Epoch [75/100], Step [700/938], d_loss: 0.1710, g_loss: 4.2698\n",
      "Epoch [75/100], Step [800/938], d_loss: 0.2704, g_loss: 4.4242\n",
      "Epoch [75/100], Step [900/938], d_loss: 0.1251, g_loss: 4.3296\n",
      "Epoch [75/100] Completed\n",
      "[==============] Average d_loss: 0.2579 - Average g_loss: 4.4231\n",
      "Epoch [76/100], Step [0/938], d_loss: 0.2525, g_loss: 4.6667\n",
      "Epoch [76/100], Step [100/938], d_loss: 0.2742, g_loss: 5.4736\n",
      "Epoch [76/100], Step [200/938], d_loss: 0.3513, g_loss: 4.3382\n",
      "Epoch [76/100], Step [300/938], d_loss: 0.3149, g_loss: 3.1411\n",
      "Epoch [76/100], Step [400/938], d_loss: 0.2570, g_loss: 4.2725\n",
      "Epoch [76/100], Step [500/938], d_loss: 0.2891, g_loss: 3.3216\n",
      "Epoch [76/100], Step [600/938], d_loss: 0.2764, g_loss: 5.2700\n",
      "Epoch [76/100], Step [700/938], d_loss: 0.2380, g_loss: 5.1207\n",
      "Epoch [76/100], Step [800/938], d_loss: 0.2802, g_loss: 3.3530\n",
      "Epoch [76/100], Step [900/938], d_loss: 0.2866, g_loss: 3.3396\n",
      "Epoch [76/100] Completed\n",
      "[==============] Average d_loss: 0.2649 - Average g_loss: 4.6082\n",
      "Epoch [77/100], Step [0/938], d_loss: 0.2641, g_loss: 3.6654\n",
      "Epoch [77/100], Step [100/938], d_loss: 0.3794, g_loss: 3.7109\n",
      "Epoch [77/100], Step [200/938], d_loss: 0.3081, g_loss: 3.8785\n",
      "Epoch [77/100], Step [300/938], d_loss: 0.1462, g_loss: 5.4842\n",
      "Epoch [77/100], Step [400/938], d_loss: 0.3578, g_loss: 5.9592\n",
      "Epoch [77/100], Step [500/938], d_loss: 0.2381, g_loss: 5.4645\n",
      "Epoch [77/100], Step [600/938], d_loss: 0.2417, g_loss: 3.5390\n",
      "Epoch [77/100], Step [700/938], d_loss: 0.2293, g_loss: 3.7992\n",
      "Epoch [77/100], Step [800/938], d_loss: 0.3619, g_loss: 3.0058\n",
      "Epoch [77/100], Step [900/938], d_loss: 0.2001, g_loss: 5.3074\n",
      "Epoch [77/100] Completed\n",
      "[==============] Average d_loss: 0.2699 - Average g_loss: 4.4842\n",
      "Epoch [78/100], Step [0/938], d_loss: 0.1478, g_loss: 4.1182\n",
      "Epoch [78/100], Step [100/938], d_loss: 0.2078, g_loss: 4.4899\n",
      "Epoch [78/100], Step [200/938], d_loss: 0.1904, g_loss: 3.3487\n",
      "Epoch [78/100], Step [300/938], d_loss: 0.3261, g_loss: 4.0198\n",
      "Epoch [78/100], Step [400/938], d_loss: 0.2563, g_loss: 4.2105\n",
      "Epoch [78/100], Step [500/938], d_loss: 0.2668, g_loss: 4.0149\n",
      "Epoch [78/100], Step [600/938], d_loss: 0.2196, g_loss: 4.9529\n",
      "Epoch [78/100], Step [700/938], d_loss: 0.2074, g_loss: 4.4372\n",
      "Epoch [78/100], Step [800/938], d_loss: 0.1772, g_loss: 5.2317\n",
      "Epoch [78/100], Step [900/938], d_loss: 0.1735, g_loss: 4.0724\n",
      "Epoch [78/100] Completed\n",
      "[==============] Average d_loss: 0.2557 - Average g_loss: 4.5004\n",
      "Epoch [79/100], Step [0/938], d_loss: 0.1350, g_loss: 5.5210\n",
      "Epoch [79/100], Step [100/938], d_loss: 0.2207, g_loss: 4.9681\n",
      "Epoch [79/100], Step [200/938], d_loss: 0.1664, g_loss: 4.6868\n",
      "Epoch [79/100], Step [300/938], d_loss: 0.1434, g_loss: 4.5416\n",
      "Epoch [79/100], Step [400/938], d_loss: 0.3042, g_loss: 3.8587\n",
      "Epoch [79/100], Step [500/938], d_loss: 0.3644, g_loss: 5.3646\n",
      "Epoch [79/100], Step [600/938], d_loss: 0.1915, g_loss: 4.1985\n",
      "Epoch [79/100], Step [700/938], d_loss: 0.2348, g_loss: 3.8799\n",
      "Epoch [79/100], Step [800/938], d_loss: 0.5603, g_loss: 6.0304\n",
      "Epoch [79/100], Step [900/938], d_loss: 0.2058, g_loss: 4.6537\n",
      "Epoch [79/100] Completed\n",
      "[==============] Average d_loss: 0.2636 - Average g_loss: 4.5508\n",
      "Epoch [80/100], Step [0/938], d_loss: 0.2228, g_loss: 3.7206\n",
      "Epoch [80/100], Step [100/938], d_loss: 0.2010, g_loss: 4.4539\n",
      "Epoch [80/100], Step [200/938], d_loss: 0.3137, g_loss: 3.8888\n",
      "Epoch [80/100], Step [300/938], d_loss: 0.1268, g_loss: 5.0462\n",
      "Epoch [80/100], Step [400/938], d_loss: 0.2461, g_loss: 4.6985\n",
      "Epoch [80/100], Step [500/938], d_loss: 0.1269, g_loss: 4.8984\n",
      "Epoch [80/100], Step [600/938], d_loss: 0.3993, g_loss: 7.4873\n",
      "Epoch [80/100], Step [700/938], d_loss: 0.2629, g_loss: 4.6031\n",
      "Epoch [80/100], Step [800/938], d_loss: 0.1810, g_loss: 4.1540\n",
      "Epoch [80/100], Step [900/938], d_loss: 0.2822, g_loss: 4.3966\n",
      "Epoch [80/100] Completed\n",
      "[==============] Average d_loss: 0.2499 - Average g_loss: 4.5702\n",
      "Epoch [81/100], Step [0/938], d_loss: 0.3514, g_loss: 3.1353\n",
      "Epoch [81/100], Step [100/938], d_loss: 0.3208, g_loss: 4.8908\n",
      "Epoch [81/100], Step [200/938], d_loss: 0.1683, g_loss: 3.5848\n",
      "Epoch [81/100], Step [300/938], d_loss: 0.2023, g_loss: 4.3477\n",
      "Epoch [81/100], Step [400/938], d_loss: 0.1929, g_loss: 4.2298\n",
      "Epoch [81/100], Step [500/938], d_loss: 0.2862, g_loss: 3.9623\n",
      "Epoch [81/100], Step [600/938], d_loss: 0.3089, g_loss: 2.8969\n",
      "Epoch [81/100], Step [700/938], d_loss: 0.1687, g_loss: 5.9001\n",
      "Epoch [81/100], Step [800/938], d_loss: 0.3309, g_loss: 6.7283\n",
      "Epoch [81/100], Step [900/938], d_loss: 0.1389, g_loss: 5.1271\n",
      "Epoch [81/100] Completed\n",
      "[==============] Average d_loss: 0.2510 - Average g_loss: 4.6843\n",
      "Epoch [82/100], Step [0/938], d_loss: 0.1931, g_loss: 5.2838\n",
      "Epoch [82/100], Step [100/938], d_loss: 0.2726, g_loss: 5.5246\n",
      "Epoch [82/100], Step [200/938], d_loss: 0.2277, g_loss: 4.2117\n",
      "Epoch [82/100], Step [300/938], d_loss: 0.2843, g_loss: 3.6443\n",
      "Epoch [82/100], Step [400/938], d_loss: 0.3109, g_loss: 3.9603\n",
      "Epoch [82/100], Step [500/938], d_loss: 0.3396, g_loss: 2.9923\n",
      "Epoch [82/100], Step [600/938], d_loss: 0.2619, g_loss: 3.4579\n",
      "Epoch [82/100], Step [700/938], d_loss: 0.2431, g_loss: 5.0663\n",
      "Epoch [82/100], Step [800/938], d_loss: 0.2838, g_loss: 4.8661\n",
      "Epoch [82/100], Step [900/938], d_loss: 0.2462, g_loss: 3.9301\n",
      "Epoch [82/100] Completed\n",
      "[==============] Average d_loss: 0.2473 - Average g_loss: 4.6152\n",
      "Epoch [83/100], Step [0/938], d_loss: 0.4008, g_loss: 4.9201\n",
      "Epoch [83/100], Step [100/938], d_loss: 0.2489, g_loss: 4.3570\n",
      "Epoch [83/100], Step [200/938], d_loss: 0.1923, g_loss: 4.0155\n",
      "Epoch [83/100], Step [300/938], d_loss: 0.3600, g_loss: 6.6137\n",
      "Epoch [83/100], Step [400/938], d_loss: 0.2040, g_loss: 4.0065\n",
      "Epoch [83/100], Step [500/938], d_loss: 0.2200, g_loss: 3.9141\n",
      "Epoch [83/100], Step [600/938], d_loss: 0.2559, g_loss: 5.7300\n",
      "Epoch [83/100], Step [700/938], d_loss: 0.3180, g_loss: 3.5165\n",
      "Epoch [83/100], Step [800/938], d_loss: 0.1667, g_loss: 5.1994\n",
      "Epoch [83/100], Step [900/938], d_loss: 0.2183, g_loss: 4.4632\n",
      "Epoch [83/100] Completed\n",
      "[==============] Average d_loss: 0.2443 - Average g_loss: 4.6459\n",
      "Epoch [84/100], Step [0/938], d_loss: 0.1023, g_loss: 4.4262\n",
      "Epoch [84/100], Step [100/938], d_loss: 0.1531, g_loss: 4.4278\n",
      "Epoch [84/100], Step [200/938], d_loss: 0.3328, g_loss: 7.0781\n",
      "Epoch [84/100], Step [300/938], d_loss: 0.3095, g_loss: 4.6701\n",
      "Epoch [84/100], Step [400/938], d_loss: 0.2366, g_loss: 5.7157\n",
      "Epoch [84/100], Step [500/938], d_loss: 0.3850, g_loss: 7.8039\n",
      "Epoch [84/100], Step [600/938], d_loss: 0.1638, g_loss: 5.9625\n",
      "Epoch [84/100], Step [700/938], d_loss: 0.2965, g_loss: 6.1301\n",
      "Epoch [84/100], Step [800/938], d_loss: 0.2490, g_loss: 4.7785\n",
      "Epoch [84/100], Step [900/938], d_loss: 0.1971, g_loss: 6.0376\n",
      "Epoch [84/100] Completed\n",
      "[==============] Average d_loss: 0.2420 - Average g_loss: 4.7496\n",
      "Epoch [85/100], Step [0/938], d_loss: 0.3626, g_loss: 3.9247\n",
      "Epoch [85/100], Step [100/938], d_loss: 0.0792, g_loss: 6.9650\n",
      "Epoch [85/100], Step [200/938], d_loss: 0.2590, g_loss: 2.8871\n",
      "Epoch [85/100], Step [300/938], d_loss: 0.3350, g_loss: 3.8763\n",
      "Epoch [85/100], Step [400/938], d_loss: 0.4151, g_loss: 3.3358\n",
      "Epoch [85/100], Step [500/938], d_loss: 0.2984, g_loss: 6.5390\n",
      "Epoch [85/100], Step [600/938], d_loss: 0.1600, g_loss: 4.9986\n",
      "Epoch [85/100], Step [700/938], d_loss: 0.3664, g_loss: 4.3064\n",
      "Epoch [85/100], Step [800/938], d_loss: 0.2593, g_loss: 3.8365\n",
      "Epoch [85/100], Step [900/938], d_loss: 0.2828, g_loss: 4.1557\n",
      "Epoch [85/100] Completed\n",
      "[==============] Average d_loss: 0.2521 - Average g_loss: 4.7503\n",
      "Epoch [86/100], Step [0/938], d_loss: 1.1548, g_loss: 5.3912\n",
      "Epoch [86/100], Step [100/938], d_loss: 0.1956, g_loss: 3.6941\n",
      "Epoch [86/100], Step [200/938], d_loss: 0.2421, g_loss: 6.0279\n",
      "Epoch [86/100], Step [300/938], d_loss: 0.2153, g_loss: 5.0700\n",
      "Epoch [86/100], Step [400/938], d_loss: 0.3029, g_loss: 3.2506\n",
      "Epoch [86/100], Step [500/938], d_loss: 0.3671, g_loss: 2.7609\n",
      "Epoch [86/100], Step [600/938], d_loss: 0.1950, g_loss: 3.7954\n",
      "Epoch [86/100], Step [700/938], d_loss: 0.1578, g_loss: 3.8961\n",
      "Epoch [86/100], Step [800/938], d_loss: 0.3293, g_loss: 2.6838\n",
      "Epoch [86/100], Step [900/938], d_loss: 0.1782, g_loss: 3.6130\n",
      "Epoch [86/100] Completed\n",
      "[==============] Average d_loss: 0.2508 - Average g_loss: 4.4598\n",
      "Epoch [87/100], Step [0/938], d_loss: 0.2313, g_loss: 5.4889\n",
      "Epoch [87/100], Step [100/938], d_loss: 0.1710, g_loss: 5.1170\n",
      "Epoch [87/100], Step [200/938], d_loss: 0.1724, g_loss: 4.0936\n",
      "Epoch [87/100], Step [300/938], d_loss: 0.2370, g_loss: 4.6264\n",
      "Epoch [87/100], Step [400/938], d_loss: 0.1593, g_loss: 4.7231\n",
      "Epoch [87/100], Step [500/938], d_loss: 0.1953, g_loss: 4.0925\n",
      "Epoch [87/100], Step [600/938], d_loss: 0.3101, g_loss: 4.1974\n",
      "Epoch [87/100], Step [700/938], d_loss: 0.2017, g_loss: 4.5123\n",
      "Epoch [87/100], Step [800/938], d_loss: 0.3208, g_loss: 3.7991\n",
      "Epoch [87/100], Step [900/938], d_loss: 0.3564, g_loss: 4.0586\n",
      "Epoch [87/100] Completed\n",
      "[==============] Average d_loss: 0.2525 - Average g_loss: 4.3693\n",
      "Epoch [88/100], Step [0/938], d_loss: 0.2452, g_loss: 4.2528\n",
      "Epoch [88/100], Step [100/938], d_loss: 0.5648, g_loss: 3.5374\n",
      "Epoch [88/100], Step [200/938], d_loss: 0.2523, g_loss: 5.1718\n",
      "Epoch [88/100], Step [300/938], d_loss: 0.3627, g_loss: 4.8096\n",
      "Epoch [88/100], Step [400/938], d_loss: 0.1409, g_loss: 5.5456\n",
      "Epoch [88/100], Step [500/938], d_loss: 0.1752, g_loss: 4.9534\n",
      "Epoch [88/100], Step [600/938], d_loss: 0.1082, g_loss: 5.2014\n",
      "Epoch [88/100], Step [700/938], d_loss: 0.2237, g_loss: 5.8740\n",
      "Epoch [88/100], Step [800/938], d_loss: 0.1820, g_loss: 4.3701\n",
      "Epoch [88/100], Step [900/938], d_loss: 0.1253, g_loss: 4.8083\n",
      "Epoch [88/100] Completed\n",
      "[==============] Average d_loss: 0.2624 - Average g_loss: 4.4896\n",
      "Epoch [89/100], Step [0/938], d_loss: 0.3059, g_loss: 4.7940\n",
      "Epoch [89/100], Step [100/938], d_loss: 0.1742, g_loss: 4.5656\n",
      "Epoch [89/100], Step [200/938], d_loss: 0.2907, g_loss: 4.1487\n",
      "Epoch [89/100], Step [300/938], d_loss: 0.2230, g_loss: 4.6862\n",
      "Epoch [89/100], Step [400/938], d_loss: 0.2332, g_loss: 4.8581\n",
      "Epoch [89/100], Step [500/938], d_loss: 0.3305, g_loss: 5.2339\n",
      "Epoch [89/100], Step [600/938], d_loss: 0.3689, g_loss: 3.3321\n",
      "Epoch [89/100], Step [700/938], d_loss: 0.1389, g_loss: 4.0261\n",
      "Epoch [89/100], Step [800/938], d_loss: 0.1841, g_loss: 4.4644\n",
      "Epoch [89/100], Step [900/938], d_loss: 0.1933, g_loss: 4.7937\n",
      "Epoch [89/100] Completed\n",
      "[==============] Average d_loss: 0.2354 - Average g_loss: 4.5439\n",
      "Epoch [90/100], Step [0/938], d_loss: 0.3942, g_loss: 5.3660\n",
      "Epoch [90/100], Step [100/938], d_loss: 0.1541, g_loss: 5.3319\n",
      "Epoch [90/100], Step [200/938], d_loss: 0.2730, g_loss: 3.7391\n",
      "Epoch [90/100], Step [300/938], d_loss: 0.1751, g_loss: 5.0746\n",
      "Epoch [90/100], Step [400/938], d_loss: 0.2027, g_loss: 4.4215\n",
      "Epoch [90/100], Step [500/938], d_loss: 0.2684, g_loss: 3.4941\n",
      "Epoch [90/100], Step [600/938], d_loss: 0.1146, g_loss: 4.9147\n",
      "Epoch [90/100], Step [700/938], d_loss: 0.1166, g_loss: 4.7237\n",
      "Epoch [90/100], Step [800/938], d_loss: 0.4685, g_loss: 7.5586\n",
      "Epoch [90/100], Step [900/938], d_loss: 0.3509, g_loss: 2.5218\n",
      "Epoch [90/100] Completed\n",
      "[==============] Average d_loss: 0.2477 - Average g_loss: 4.6736\n",
      "Epoch [91/100], Step [0/938], d_loss: 0.3746, g_loss: 5.2625\n",
      "Epoch [91/100], Step [100/938], d_loss: 0.2348, g_loss: 3.8110\n",
      "Epoch [91/100], Step [200/938], d_loss: 0.2884, g_loss: 3.7859\n",
      "Epoch [91/100], Step [300/938], d_loss: 0.1752, g_loss: 4.5166\n",
      "Epoch [91/100], Step [400/938], d_loss: 0.2033, g_loss: 4.7330\n",
      "Epoch [91/100], Step [500/938], d_loss: 0.4768, g_loss: 5.8837\n",
      "Epoch [91/100], Step [600/938], d_loss: 0.3159, g_loss: 3.8661\n",
      "Epoch [91/100], Step [700/938], d_loss: 0.3816, g_loss: 4.7523\n",
      "Epoch [91/100], Step [800/938], d_loss: 0.1670, g_loss: 4.2251\n",
      "Epoch [91/100], Step [900/938], d_loss: 0.1346, g_loss: 4.1740\n",
      "Epoch [91/100] Completed\n",
      "[==============] Average d_loss: 0.2494 - Average g_loss: 4.5862\n",
      "Epoch [92/100], Step [0/938], d_loss: 0.1889, g_loss: 5.0721\n",
      "Epoch [92/100], Step [100/938], d_loss: 0.2409, g_loss: 4.6214\n",
      "Epoch [92/100], Step [200/938], d_loss: 0.3865, g_loss: 4.6722\n",
      "Epoch [92/100], Step [300/938], d_loss: 0.2929, g_loss: 5.7097\n",
      "Epoch [92/100], Step [400/938], d_loss: 0.2524, g_loss: 4.2900\n",
      "Epoch [92/100], Step [500/938], d_loss: 0.3055, g_loss: 4.6257\n",
      "Epoch [92/100], Step [600/938], d_loss: 0.1477, g_loss: 4.7801\n",
      "Epoch [92/100], Step [700/938], d_loss: 0.2005, g_loss: 4.1144\n",
      "Epoch [92/100], Step [800/938], d_loss: 0.1640, g_loss: 3.7207\n",
      "Epoch [92/100], Step [900/938], d_loss: 0.2279, g_loss: 4.1323\n",
      "Epoch [92/100] Completed\n",
      "[==============] Average d_loss: 0.2428 - Average g_loss: 4.4135\n",
      "Epoch [93/100], Step [0/938], d_loss: 0.3025, g_loss: 4.6237\n",
      "Epoch [93/100], Step [100/938], d_loss: 0.1188, g_loss: 4.4699\n",
      "Epoch [93/100], Step [200/938], d_loss: 0.1601, g_loss: 4.8669\n",
      "Epoch [93/100], Step [300/938], d_loss: 0.1789, g_loss: 4.2748\n",
      "Epoch [93/100], Step [400/938], d_loss: 0.2293, g_loss: 5.1348\n",
      "Epoch [93/100], Step [500/938], d_loss: 0.2520, g_loss: 3.7902\n",
      "Epoch [93/100], Step [600/938], d_loss: 0.2111, g_loss: 3.9983\n",
      "Epoch [93/100], Step [700/938], d_loss: 0.1560, g_loss: 4.4661\n",
      "Epoch [93/100], Step [800/938], d_loss: 0.2316, g_loss: 3.4055\n",
      "Epoch [93/100], Step [900/938], d_loss: 0.3857, g_loss: 3.2688\n",
      "Epoch [93/100] Completed\n",
      "[==============] Average d_loss: 0.2443 - Average g_loss: 4.4705\n",
      "Epoch [94/100], Step [0/938], d_loss: 0.2930, g_loss: 3.6917\n",
      "Epoch [94/100], Step [100/938], d_loss: 0.1595, g_loss: 4.8653\n",
      "Epoch [94/100], Step [200/938], d_loss: 0.1886, g_loss: 4.0203\n",
      "Epoch [94/100], Step [300/938], d_loss: 0.1804, g_loss: 3.5119\n",
      "Epoch [94/100], Step [400/938], d_loss: 0.1870, g_loss: 4.6978\n",
      "Epoch [94/100], Step [500/938], d_loss: 0.2159, g_loss: 4.1757\n",
      "Epoch [94/100], Step [600/938], d_loss: 0.2929, g_loss: 4.0655\n",
      "Epoch [94/100], Step [700/938], d_loss: 0.0973, g_loss: 5.4917\n",
      "Epoch [94/100], Step [800/938], d_loss: 0.5963, g_loss: 7.6442\n",
      "Epoch [94/100], Step [900/938], d_loss: 0.1971, g_loss: 5.2704\n",
      "Epoch [94/100] Completed\n",
      "[==============] Average d_loss: 0.2478 - Average g_loss: 4.4164\n",
      "Epoch [95/100], Step [0/938], d_loss: 0.2683, g_loss: 3.0312\n",
      "Epoch [95/100], Step [100/938], d_loss: 0.1610, g_loss: 4.6488\n",
      "Epoch [95/100], Step [200/938], d_loss: 0.2973, g_loss: 2.8920\n",
      "Epoch [95/100], Step [300/938], d_loss: 0.2397, g_loss: 3.8258\n",
      "Epoch [95/100], Step [400/938], d_loss: 0.2231, g_loss: 3.6846\n",
      "Epoch [95/100], Step [500/938], d_loss: 0.3592, g_loss: 4.1906\n",
      "Epoch [95/100], Step [600/938], d_loss: 0.1001, g_loss: 4.9974\n",
      "Epoch [95/100], Step [700/938], d_loss: 0.2388, g_loss: 3.7596\n",
      "Epoch [95/100], Step [800/938], d_loss: 0.1306, g_loss: 4.6839\n",
      "Epoch [95/100], Step [900/938], d_loss: 0.2042, g_loss: 3.9484\n",
      "Epoch [95/100] Completed\n",
      "[==============] Average d_loss: 0.2653 - Average g_loss: 4.4966\n",
      "Epoch [96/100], Step [0/938], d_loss: 0.2607, g_loss: 4.2831\n",
      "Epoch [96/100], Step [100/938], d_loss: 0.3193, g_loss: 3.9863\n",
      "Epoch [96/100], Step [200/938], d_loss: 0.2950, g_loss: 3.9221\n",
      "Epoch [96/100], Step [300/938], d_loss: 0.1956, g_loss: 4.8913\n",
      "Epoch [96/100], Step [400/938], d_loss: 0.1364, g_loss: 4.1975\n",
      "Epoch [96/100], Step [500/938], d_loss: 0.5366, g_loss: 7.9298\n",
      "Epoch [96/100], Step [600/938], d_loss: 0.2514, g_loss: 3.7501\n",
      "Epoch [96/100], Step [700/938], d_loss: 0.1359, g_loss: 5.1189\n",
      "Epoch [96/100], Step [800/938], d_loss: 0.3287, g_loss: 3.3388\n",
      "Epoch [96/100], Step [900/938], d_loss: 0.4033, g_loss: 4.9917\n",
      "Epoch [96/100] Completed\n",
      "[==============] Average d_loss: 0.2577 - Average g_loss: 4.4669\n",
      "Epoch [97/100], Step [0/938], d_loss: 0.2687, g_loss: 4.7701\n",
      "Epoch [97/100], Step [100/938], d_loss: 0.2499, g_loss: 4.8291\n",
      "Epoch [97/100], Step [200/938], d_loss: 0.1629, g_loss: 4.5976\n",
      "Epoch [97/100], Step [300/938], d_loss: 0.4652, g_loss: 6.8188\n",
      "Epoch [97/100], Step [400/938], d_loss: 0.2481, g_loss: 4.7756\n",
      "Epoch [97/100], Step [500/938], d_loss: 0.2166, g_loss: 4.0576\n",
      "Epoch [97/100], Step [600/938], d_loss: 0.1271, g_loss: 4.7761\n",
      "Epoch [97/100], Step [700/938], d_loss: 0.2256, g_loss: 4.2598\n",
      "Epoch [97/100], Step [800/938], d_loss: 0.3706, g_loss: 3.3072\n",
      "Epoch [97/100], Step [900/938], d_loss: 0.2689, g_loss: 3.6056\n",
      "Epoch [97/100] Completed\n",
      "[==============] Average d_loss: 0.2506 - Average g_loss: 4.4354\n",
      "Epoch [98/100], Step [0/938], d_loss: 0.3630, g_loss: 3.4678\n",
      "Epoch [98/100], Step [100/938], d_loss: 0.1526, g_loss: 4.1440\n",
      "Epoch [98/100], Step [200/938], d_loss: 0.1336, g_loss: 3.9748\n",
      "Epoch [98/100], Step [300/938], d_loss: 0.2144, g_loss: 4.9465\n",
      "Epoch [98/100], Step [400/938], d_loss: 0.2343, g_loss: 5.3199\n",
      "Epoch [98/100], Step [500/938], d_loss: 0.3163, g_loss: 2.8677\n",
      "Epoch [98/100], Step [600/938], d_loss: 0.2350, g_loss: 5.3329\n",
      "Epoch [98/100], Step [700/938], d_loss: 0.2703, g_loss: 3.7732\n",
      "Epoch [98/100], Step [800/938], d_loss: 0.2494, g_loss: 4.6334\n",
      "Epoch [98/100], Step [900/938], d_loss: 0.1238, g_loss: 5.0661\n",
      "Epoch [98/100] Completed\n",
      "[==============] Average d_loss: 0.2556 - Average g_loss: 4.5018\n",
      "Epoch [99/100], Step [0/938], d_loss: 0.3356, g_loss: 3.0486\n",
      "Epoch [99/100], Step [100/938], d_loss: 0.2504, g_loss: 3.5744\n",
      "Epoch [99/100], Step [200/938], d_loss: 0.1533, g_loss: 4.8143\n",
      "Epoch [99/100], Step [300/938], d_loss: 0.2913, g_loss: 3.6780\n",
      "Epoch [99/100], Step [400/938], d_loss: 0.2854, g_loss: 4.4435\n",
      "Epoch [99/100], Step [500/938], d_loss: 0.3185, g_loss: 5.6841\n",
      "Epoch [99/100], Step [600/938], d_loss: 0.3220, g_loss: 4.1901\n",
      "Epoch [99/100], Step [700/938], d_loss: 0.1829, g_loss: 4.9400\n",
      "Epoch [99/100], Step [800/938], d_loss: 0.3207, g_loss: 4.1366\n",
      "Epoch [99/100], Step [900/938], d_loss: 0.5932, g_loss: 4.5328\n",
      "Epoch [99/100] Completed\n",
      "[==============] Average d_loss: 0.2549 - Average g_loss: 4.4428\n",
      "Epoch [100/100], Step [0/938], d_loss: 0.3979, g_loss: 4.5944\n",
      "Epoch [100/100], Step [100/938], d_loss: 0.2540, g_loss: 3.3790\n",
      "Epoch [100/100], Step [200/938], d_loss: 0.2532, g_loss: 4.2376\n",
      "Epoch [100/100], Step [300/938], d_loss: 0.2357, g_loss: 5.7034\n",
      "Epoch [100/100], Step [400/938], d_loss: 0.3064, g_loss: 5.0638\n",
      "Epoch [100/100], Step [500/938], d_loss: 0.2459, g_loss: 5.2243\n",
      "Epoch [100/100], Step [600/938], d_loss: 0.3573, g_loss: 2.9478\n",
      "Epoch [100/100], Step [700/938], d_loss: 0.2260, g_loss: 3.6215\n",
      "Epoch [100/100], Step [800/938], d_loss: 0.2330, g_loss: 4.4717\n",
      "Epoch [100/100], Step [900/938], d_loss: 0.4071, g_loss: 4.9469\n",
      "Epoch [100/100] Completed\n",
      "[==============] Average d_loss: 0.2579 - Average g_loss: 4.2953\n"
     ]
    }
   ],
   "source": [
    "# Define the discriminator training function\n",
    "def train_discriminator(real_samples, fake_samples, real_labels, fake_labels):\n",
    "    \"\"\"\n",
    "    Train the discriminator of a Generative Adversarial Network (GAN).\n",
    "\n",
    "    This function updates the discriminator by training it on both real and fake samples.\n",
    "    It calculates the loss for both real and fake predictions and backpropagates the total loss\n",
    "    to update the discriminator's weights.\n",
    "\n",
    "    Parameters:\n",
    "    - real_samples (Tensor): A batch of real samples from the dataset.\n",
    "    - fake_samples (Tensor): A batch of fake samples generated by the GAN's generator.\n",
    "    - real_labels (Tensor): A batch of labels, typically ones, representing real samples.\n",
    "    - fake_labels (Tensor): A batch of labels, typically zeros, representing fake samples.\n",
    "\n",
    "    The function assumes that the discriminator and loss_function are globally accessible,\n",
    "    and it also utilizes the optimizer_discriminator for the backpropagation process.\n",
    "\n",
    "    Returns:\n",
    "    - float: The total loss incurred by the discriminator for the current batch of real and fake samples.\n",
    "    \"\"\"\n",
    "    optimizer_discriminator.zero_grad()\n",
    "\n",
    "    real_predicted = discriminator(real_samples)\n",
    "    fake_predicted = discriminator(fake_samples.detach())\n",
    "\n",
    "    real_predicted_loss = loss_function(real_predicted, real_labels)\n",
    "    fake_predicted_loss = loss_function(fake_predicted, fake_labels)\n",
    "\n",
    "    total_discriminator_loss = real_predicted_loss + fake_predicted_loss\n",
    "\n",
    "    total_discriminator_loss.backward()\n",
    "    optimizer_discriminator.step()\n",
    "\n",
    "    return total_discriminator_loss.item()\n",
    "\n",
    "\n",
    "# Define the generator training function\n",
    "def train_generator(fake_samples, real_labels):\n",
    "    \"\"\"\n",
    "    Train the generator of a Generative Adversarial Network (GAN).\n",
    "\n",
    "    This function trains the generator by attempting to fool the discriminator. It updates\n",
    "    the generator based on how well it can trick the discriminator into classifying the\n",
    "    generated (fake) samples as real. The function calculates the loss by comparing the\n",
    "    discriminator's predictions on the fake samples against the 'real' labels and then\n",
    "    performs backpropagation to update the generator's weights.\n",
    "\n",
    "    Parameters:\n",
    "    - fake_samples (Tensor): A batch of fake samples generated by the GAN's generator.\n",
    "    - real_labels (Tensor): A batch of labels, typically ones, used as targets for training the generator.\n",
    "\n",
    "    This function assumes the availability of a globally accessible discriminator model,\n",
    "    a loss function, and the optimizer for the generator (optimizer_generator).\n",
    "\n",
    "    Returns:\n",
    "    - float: The loss incurred by the generator for the current batch of fake samples, indicating\n",
    "             how well the generator is able to fool the discriminator.\n",
    "    \"\"\"\n",
    "    optimizer_generator.zero_grad()\n",
    "\n",
    "    fake_predict = discriminator(fake_samples)\n",
    "\n",
    "    generated_loss = loss_function(fake_predict, real_labels)\n",
    "\n",
    "    generated_loss.backward()\n",
    "    optimizer_generator.step()\n",
    "\n",
    "    return generated_loss.item()\n",
    "\n",
    "\n",
    "def train_gan():\n",
    "    \"\"\"\n",
    "    Train a Generative Adversarial Network (GAN) consisting of a generator and discriminator.\n",
    "\n",
    "    This function orchestrates the training process of a GAN over a specified number of epochs.\n",
    "    Training involves alternating between training the discriminator and the generator.\n",
    "    The discriminator is trained to distinguish real data from fake data generated by the generator,\n",
    "    while the generator is trained to produce data that appears real to the discriminator.\n",
    "\n",
    "    Parameters:\n",
    "    - epochs (int): The number of training epochs.\n",
    "    - latent_space (int): The size of the latent space used to generate noise samples for the generator.\n",
    "    - print_interval (int): Interval of steps for printing training progress within each epoch.\n",
    "    - dataloader (DataLoader): DataLoader object providing access to the dataset.\n",
    "    - device (torch.device): The device (CPU/GPU) on which the training is performed.\n",
    "\n",
    "    The function assumes that 'train_discriminator' and 'train_generator' are pre-defined functions\n",
    "    that handle the training of the discriminator and generator, respectively. Additionally,\n",
    "    'generator' and 'discriminator' should be predefined models. The function also requires\n",
    "    'np.mean' for calculating average losses, and it prints the training progress at regular intervals.\n",
    "\n",
    "    Returns:\n",
    "    None: This function does not return a value but prints the training progress and average losses per epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    epochs = 100\n",
    "    latent_space = 100\n",
    "    print_interval = 100\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        discriminator_loss = []\n",
    "        generator_loss = []\n",
    "\n",
    "        for i, (real_samples, _) in enumerate(dataloader):\n",
    "            real_samples = real_samples.to(device)\n",
    "            batch_size = real_samples.shape[0]\n",
    "\n",
    "            # Generate noise samples and fake samples\n",
    "            noise_samples = torch.randn(batch_size, latent_space).to(device)\n",
    "            fake_samples = generator(noise_samples)\n",
    "\n",
    "            # Define labels for real and fake samples\n",
    "            real_labels = torch.ones(batch_size, 1).to(device)\n",
    "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss = train_discriminator(\n",
    "                real_samples=real_samples,\n",
    "                fake_samples=fake_samples,\n",
    "                real_labels=real_labels,\n",
    "                fake_labels=fake_labels,\n",
    "            )\n",
    "\n",
    "            # Train the generator\n",
    "            g_loss = train_generator(fake_samples=fake_samples, real_labels=real_labels)\n",
    "\n",
    "            discriminator_loss.append(d_loss)\n",
    "            generator_loss.append(g_loss)\n",
    "\n",
    "            # Print training progress every 'print_interval' iterations\n",
    "            if i % print_interval == 0:\n",
    "                print(\n",
    "                    f\"Epoch [{epoch+1}/{epochs}], Step [{i}/{len(dataloader)}], d_loss: {d_loss:.4f}, g_loss: {g_loss:.4f}\"\n",
    "                )\n",
    "\n",
    "        # Output average loss at the end of each epoch\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}] Completed\")\n",
    "        print(\n",
    "            f\"[==============] Average d_loss: {np.mean(discriminator_loss):.4f} - Average g_loss: {np.mean(generator_loss):.4f}\"\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "train_gan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "noise_data = torch.randn(1, 100)\n",
    "generated_samples = generator(noise_data.to(device))\n",
    "generated_samples = generated_samples.detach()\n",
    "\n",
    "plt.imshow(generated_samples.detach().cpu().view(28, 28), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPSG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
